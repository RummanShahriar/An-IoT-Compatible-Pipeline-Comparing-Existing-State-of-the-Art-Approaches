{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Explainable AI (XAI)"
      ],
      "metadata": {
        "id": "ujxrm5SHgwIq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation"
      ],
      "metadata": {
        "id": "jP-qnvAjkTX0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß† Step 1: What Explainable AI (XAI) and Interpretability Mean Here\n",
        "\n",
        "In your CKD project context:\n",
        "\n",
        "Interpretability = understanding how and why the model makes its predictions.\n",
        "\n",
        "Explainability = providing transparent, human-readable reasoning for each prediction (e.g., why a patient is predicted CKD positive).\n",
        "\n",
        "In medical applications, this is critical for trust, clinical validation, and deployment readiness."
      ],
      "metadata": {
        "id": "nWT5GvCvyOMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- XAI Implementation Code"
      ],
      "metadata": {
        "id": "enj28n24kecs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "file_link = \"https://drive.google.com/file/d/1InWoda-GWlyozroaVWzsj8rKuFg2bmeZ/view?usp=drive_link\"\n",
        "id = file_link.split(\"/\")[-2]\n",
        "new_link = f'https://drive.google.com/uc?id={id}'\n",
        "df_20k = pd.read_csv(new_link)\n",
        "df = df_20k.copy()"
      ],
      "metadata": {
        "id": "eHZhhe6M2Q7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # =====================================\n",
        "# # EXPLAINABLE AI (XAI) IMPLEMENTATION\n",
        "# # For CKD Detection Project\n",
        "# # =====================================\n",
        "\n",
        "# # Import required libraries\n",
        "# import shap\n",
        "# from lime.lime_tabular import LimeTabularExplainer\n",
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from imblearn.over_sampling import SMOTE\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from sklearn.ensemble import StackingClassifier\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.neural_network import MLPClassifier\n",
        "# from sklearn.metrics import classification_report\n",
        "# from sklearn.ensemble import GradientBoostingClassifier\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
        "# from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# # =====================================\n",
        "# # 1Ô∏è‚É£ Data Preparation (Same as before)\n",
        "# # =====================================\n",
        "\n",
        "# # Assume df is your CKD dataset (already cleaned)\n",
        "# # Example: df = pd.read_csv(\"CKD_preprocessed.csv\")\n",
        "\n",
        "# # Split features and labels\n",
        "# X = df.drop(\"target\", axis=1)\n",
        "# y = df[\"target\"]\n",
        "\n",
        "# # Apply SMOTE and Scaling\n",
        "# smote = SMOTE(random_state=42)\n",
        "# X_res, y_res = smote.fit_resample(X, y)\n",
        "\n",
        "# scaler = StandardScaler()\n",
        "# X_scaled = scaler.fit_transform(X_res)\n",
        "\n",
        "# # Apply RF feature selection (12 features)\n",
        "# rf_selector = RandomForestClassifier(random_state=42)\n",
        "# rf_selector.fit(X_scaled, y_res)\n",
        "# importances = rf_selector.feature_importances_\n",
        "# indices = np.argsort(importances)[::-1][:12]\n",
        "# selected_features = X.columns[indices]\n",
        "\n",
        "# X_selected = X_res[selected_features]\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X_selected, y_res, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Scale again after feature selection\n",
        "# scaler2 = StandardScaler()\n",
        "# X_train_scaled = scaler2.fit_transform(X_train)\n",
        "# X_test_scaled = scaler2.transform(X_test)\n",
        "\n",
        "# # =====================================\n",
        "# # 2Ô∏è‚É£ Random Forest Model (Traditional)\n",
        "# # =====================================\n",
        "# rf_model = RandomForestClassifier(random_state=42)\n",
        "# rf_model.fit(X_train_scaled, y_train)\n",
        "# y_pred_rf = rf_model.predict(X_test_scaled)\n",
        "\n",
        "# print(\"\\nüéØ Random Forest Performance:\")\n",
        "# print(classification_report(y_test, y_pred_rf))\n",
        "\n",
        "# # ---------- SHAP (RF) ----------\n",
        "# print(\"\\nüîç SHAP Analysis for Random Forest...\")\n",
        "# explainer_rf = shap.TreeExplainer(rf_model)\n",
        "# shap_values_rf = explainer_rf.shap_values(X_test_scaled)\n",
        "\n",
        "# # Global summary plot\n",
        "# shap.summary_plot(shap_values_rf[1], X_test_scaled, feature_names=selected_features, show=False)\n",
        "# plt.title(\"SHAP Feature Importance - Random Forest\")\n",
        "# plt.show()\n",
        "\n",
        "# # Local explanation for one instance\n",
        "# shap.force_plot(explainer_rf.expected_value[1], shap_values_rf[1][0,:],\n",
        "#                 X_test_scaled[0,:], feature_names=selected_features, matplotlib=True)\n",
        "\n",
        "# # ---------- LIME (RF) ----------\n",
        "# print(\"\\nüí° LIME Explanation for Random Forest...\")\n",
        "# lime_explainer = LimeTabularExplainer(X_train_scaled,\n",
        "#                                       feature_names=selected_features,\n",
        "#                                       class_names=['No CKD', 'CKD'],\n",
        "#                                       mode='classification')\n",
        "# lime_exp = lime_explainer.explain_instance(X_test_scaled[0],\n",
        "#                                            rf_model.predict_proba,\n",
        "#                                            num_features=10)\n",
        "# lime_exp.show_in_notebook(show_table=True)\n",
        "\n",
        "# # =====================================\n",
        "# # 3Ô∏è‚É£ Stacking Ensemble Model\n",
        "# # =====================================\n",
        "# print(\"\\nüèóÔ∏è Training Stacking Ensemble...\")\n",
        "# base_estimators = [\n",
        "#     ('lr', LogisticRegression(max_iter=1000)),\n",
        "#     ('mlp', MLPClassifier(max_iter=500)),\n",
        "#     ('gb', GradientBoostingClassifier())\n",
        "# ]\n",
        "# stack_model = StackingClassifier(estimators=base_estimators, final_estimator=LogisticRegression())\n",
        "# stack_model.fit(X_train_scaled, y_train)\n",
        "# y_pred_stack = stack_model.predict(X_test_scaled)\n",
        "\n",
        "# print(\"\\nüéØ Stacking Ensemble Performance:\")\n",
        "# print(classification_report(y_test, y_pred_stack))\n",
        "\n",
        "# # ---------- SHAP (Stacking) ----------\n",
        "# print(\"\\nüîç SHAP Analysis for Stacking Ensemble...\")\n",
        "# explainer_stack = shap.Explainer(stack_model, X_train_scaled)\n",
        "# shap_values_stack = explainer_stack(X_test_scaled)\n",
        "# shap.summary_plot(shap_values_stack, X_test_scaled, feature_names=selected_features, show=False)\n",
        "# plt.title(\"SHAP Summary - Stacking Ensemble\")\n",
        "# plt.show()\n",
        "\n",
        "# # ---------- LIME (Stacking) ----------\n",
        "# print(\"\\nüí° LIME Explanation for Stacking Ensemble...\")\n",
        "# lime_exp_stack = lime_explainer.explain_instance(X_test_scaled[1],\n",
        "#                                                  stack_model.predict_proba,\n",
        "#                                                  num_features=10)\n",
        "# lime_exp_stack.show_in_notebook(show_table=True)\n",
        "\n",
        "# # =====================================\n",
        "# # 4Ô∏è‚É£ Deep NN Bottleneck Model\n",
        "# # =====================================\n",
        "# print(\"\\nüß¨ Training Deep NN Bottleneck Model...\")\n",
        "\n",
        "# input_dim = X_train_scaled.shape[1]\n",
        "\n",
        "# dnn_model = Sequential([\n",
        "#     Dense(512, activation='relu', input_dim=input_dim),\n",
        "#     BatchNormalization(),\n",
        "#     Dropout(0.5),\n",
        "#     Dense(256, activation='relu'),\n",
        "#     BatchNormalization(),\n",
        "#     Dropout(0.4),\n",
        "#     Dense(128, activation='relu'),\n",
        "#     BatchNormalization(),\n",
        "#     Dropout(0.3),\n",
        "#     Dense(64, activation='relu'),\n",
        "#     BatchNormalization(),\n",
        "#     Dropout(0.3),\n",
        "#     Dense(1, activation='sigmoid')\n",
        "# ])\n",
        "\n",
        "# dnn_model.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# dnn_model.fit(X_train_scaled, y_train, validation_data=(X_test_scaled, y_test),\n",
        "#               epochs=50, batch_size=32, verbose=1)\n",
        "\n",
        "# # ---------- SHAP (DNN) ----------\n",
        "# print(\"\\nüîç SHAP DeepExplainer for Bottleneck NN...\")\n",
        "# background = X_train_scaled[np.random.choice(X_train_scaled.shape[0], 100, replace=False)]\n",
        "# explainer_dnn = shap.DeepExplainer(dnn_model, background)\n",
        "# shap_values_dnn = explainer_dnn.shap_values(X_test_scaled[:100])\n",
        "\n",
        "# shap.summary_plot(shap_values_dnn[0], X_test_scaled[:100],\n",
        "#                   feature_names=selected_features, show=False)\n",
        "# plt.title(\"SHAP Summary - Deep NN Bottleneck\")\n",
        "# plt.show()\n",
        "\n",
        "# # ---------- LIME (DNN) ----------\n",
        "# print(\"\\nüí° LIME Explanation for Deep NN Bottleneck...\")\n",
        "# lime_exp_dnn = lime_explainer.explain_instance(X_test_scaled[2],\n",
        "#                                                dnn_model.predict,\n",
        "#                                                num_features=10)\n",
        "# lime_exp_dnn.show_in_notebook(show_table=True)\n",
        "\n",
        "# print(\"\\n‚úÖ XAI Implementation Completed Successfully!\")\n"
      ],
      "metadata": {
        "id": "AYrgeeHo2co9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93e153cd",
        "outputId": "fcae092d-98f7-4adb-fccc-efbde4b17d98"
      },
      "source": [
        "!pip install lime"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lime\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/275.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from lime) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from lime) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from lime) (1.16.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from lime) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.12/dist-packages (from lime) (1.6.1)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.12/dist-packages (from lime) (0.25.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (3.5)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (11.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (2025.10.16)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.18->lime) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.18->lime) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.17.0)\n",
            "Building wheels for collected packages: lime\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283834 sha256=f0def0624cddd9c5c01a631e25f4ba4f4d7b0cd1f3e2b630609a84193f747c14\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/5d/0e/4b4fff9a47468fed5633211fb3b76d1db43fe806a17fb7486a\n",
            "Successfully built lime\n",
            "Installing collected packages: lime\n",
            "Successfully installed lime-0.2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e03ab62"
      },
      "source": [
        "I've added a cell to install the `lime` library. Once that cell finishes running, the import error should be resolved."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e846af4b"
      },
      "source": [
        "# =====================================\n",
        "# EXPLAINABLE AI (XAI) IMPLEMENTATION\n",
        "# For CKD Detection Project\n",
        "# =====================================\n",
        "\n",
        "# Import required libraries\n",
        "import shap\n",
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# =====================================\n",
        "# 1Ô∏è‚É£ Data Preparation (Same as before)\n",
        "# =====================================\n",
        "\n",
        "# Assume df is your CKD dataset (already cleaned)\n",
        "# Example: df = pd.read_csv(\"CKD_preprocessed.csv\")\n",
        "\n",
        "# Split features and labels\n",
        "X = df.drop(\"classification\", axis=1)\n",
        "y = df[\"classification\"]\n",
        "\n",
        "# Apply SMOTE and Scaling\n",
        "smote = SMOTE(random_state=42)\n",
        "X_res, y_res = smote.fit_resample(X, y)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_res)\n",
        "\n",
        "# Apply RF feature selection (12 features)\n",
        "rf_selector = RandomForestClassifier(random_state=42)\n",
        "rf_selector.fit(X_scaled, y_res)\n",
        "importances = rf_selector.feature_importances_\n",
        "indices = np.argsort(importances)[::-1][:12]\n",
        "selected_features = X.columns[indices]\n",
        "\n",
        "X_selected = X_res[selected_features]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y_res, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale again after feature selection\n",
        "scaler2 = StandardScaler()\n",
        "X_train_scaled = scaler2.fit_transform(X_train)\n",
        "X_test_scaled = scaler2.transform(X_test)\n",
        "\n",
        "# =====================================\n",
        "# 2Ô∏è‚É£ Random Forest Model (Traditional)\n",
        "# =====================================\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(X_train_scaled, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test_scaled)\n",
        "\n",
        "print(\"\\nüéØ Random Forest Performance:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "\n",
        "# ---------- SHAP (RF) ----------\n",
        "print(\"\\nüîç SHAP Analysis for Random Forest...\")\n",
        "explainer_rf = shap.TreeExplainer(rf_model)\n",
        "shap_values_rf = explainer_rf.shap_values(X_test_scaled)\n",
        "\n",
        "# Global summary plot\n",
        "shap.summary_plot(shap_values_rf[1], X_test_scaled, feature_names=selected_features, show=False)\n",
        "plt.title(\"SHAP Feature Importance - Random Forest\")\n",
        "plt.show()\n",
        "\n",
        "# Local explanation for one instance\n",
        "shap.force_plot(explainer_rf.expected_value[1], shap_values_rf[1][0,:],\n",
        "                X_test_scaled[0,:], feature_names=selected_features, matplotlib=True)\n",
        "\n",
        "# ---------- LIME (RF) ----------\n",
        "print(\"\\nüí° LIME Explanation for Random Forest...\")\n",
        "lime_explainer = LimeTabularExplainer(X_train_scaled,\n",
        "                                      feature_names=selected_features,\n",
        "                                      class_names=['No CKD', 'CKD'],\n",
        "                                      mode='classification')\n",
        "lime_exp = lime_explainer.explain_instance(X_test_scaled[0],\n",
        "                                           rf_model.predict_proba,\n",
        "                                           num_features=10)\n",
        "lime_exp.show_in_notebook(show_table=True)\n",
        "\n",
        "# =====================================\n",
        "# 3Ô∏è‚É£ Stacking Ensemble Model\n",
        "# =====================================\n",
        "print(\"\\nüèóÔ∏è Training Stacking Ensemble...\")\n",
        "base_estimators = [\n",
        "    ('lr', LogisticRegression(max_iter=1000)),\n",
        "    ('mlp', MLPClassifier(max_iter=500)),\n",
        "    ('gb', GradientBoostingClassifier())\n",
        "]\n",
        "stack_model = StackingClassifier(estimators=base_estimators, final_estimator=LogisticRegression())\n",
        "stack_model.fit(X_train_scaled, y_train)\n",
        "y_pred_stack = stack_model.predict(X_test_scaled)\n",
        "\n",
        "print(\"\\nüéØ Stacking Ensemble Performance:\")\n",
        "print(classification_report(y_test, y_pred_stack))\n",
        "\n",
        "# ---------- SHAP (Stacking) ----------\n",
        "print(\"\\nüîç SHAP Analysis for Stacking Ensemble...\")\n",
        "explainer_stack = shap.Explainer(stack_model, X_train_scaled)\n",
        "shap_values_stack = explainer_stack(X_test_scaled)\n",
        "shap.summary_plot(shap_values_stack, X_test_scaled, feature_names=selected_features, show=False)\n",
        "plt.title(\"SHAP Summary - Stacking Ensemble\")\n",
        "plt.show()\n",
        "\n",
        "# ---------- LIME (Stacking) ----------\n",
        "print(\"\\nüí° LIME Explanation for Stacking Ensemble...\")\n",
        "lime_exp_stack = lime_explainer.explain_instance(X_test_scaled[1],\n",
        "                                                 stack_model.predict_proba,\n",
        "                                                 num_features=10)\n",
        "lime_exp_stack.show_in_notebook(show_table=True)\n",
        "\n",
        "# =====================================\n",
        "# 4Ô∏è‚É£ Deep NN Bottleneck Model\n",
        "# =====================================\n",
        "print(\"\\nüß¨ Training Deep NN Bottleneck Model...\")\n",
        "\n",
        "input_dim = X_train_scaled.shape[1]\n",
        "\n",
        "dnn_model = Sequential([\n",
        "    Dense(512, activation='relu', input_dim=input_dim),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(256, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.4),\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "dnn_model.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "dnn_model.fit(X_train_scaled, y_train, validation_data=(X_test_scaled, y_test),\n",
        "              epochs=50, batch_size=32, verbose=1)\n",
        "\n",
        "# ---------- SHAP (DNN) ----------\n",
        "print(\"\\nüîç SHAP DeepExplainer for Bottleneck NN...\")\n",
        "background = X_train_scaled[np.random.choice(X_train_scaled.shape[0], 100, replace=False)]\n",
        "explainer_dnn = shap.DeepExplainer(dnn_model, background)\n",
        "shap_values_dnn = explainer_dnn.shap_values(X_test_scaled[:100])\n",
        "\n",
        "shap.summary_plot(shap_values_dnn[0], X_test_scaled[:100],\n",
        "                  feature_names=selected_features, show=False)\n",
        "plt.title(\"SHAP Summary - Deep NN Bottleneck\")\n",
        "plt.show()\n",
        "\n",
        "# ---------- LIME (DNN) ----------\n",
        "print(\"\\nüí° LIME Explanation for Deep NN Bottleneck...\")\n",
        "lime_exp_dnn = lime_explainer.explain_instance(X_test_scaled[2],\n",
        "                                               dnn_model.predict,\n",
        "                                               num_features=10)\n",
        "lime_exp_dnn.show_in_notebook(show_table=True)\n",
        "\n",
        "print(\"\\n‚úÖ XAI Implementation Completed Successfully!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Separated"
      ],
      "metadata": {
        "id": "ivE2zahBzq1Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Block 1 ‚Äî Random Forest (uses explicit selected_features list)"
      ],
      "metadata": {
        "id": "wAGr33RN8yB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# Random Forest (uses explicit selected_features)\n",
        "# ------------------------------\n",
        "import shap\n",
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Ensure your df is loaded and target column name\n",
        "target_col = \"classification\"   # change if needed\n",
        "\n",
        "# Selected features list (use exactly as you provided)\n",
        "selected_features = [\n",
        "    'blood pressure', 'specific gravity', 'albumin', 'sugar', 'blood glucose random',\n",
        "    'blood urea', 'sodium', 'potassium', 'hemoglobin', 'packed cell volume',\n",
        "    'white blood cell count', 'red blood cell count'\n",
        "]\n",
        "\n",
        "# Prepare data: use only the selected features for RF\n",
        "X = df[selected_features].copy()\n",
        "y = df[target_col].astype(int)\n",
        "\n",
        "# Optionally handle missing values prior to SMOTE if present:\n",
        "# X = X.fillna(X.median())\n",
        "\n",
        "# SMOTE (applied here to the selected-feature subset)\n",
        "smote = SMOTE(random_state=42)\n",
        "X_res, y_res = smote.fit_resample(X, y)\n",
        "\n",
        "# Scale\n",
        "scaler = StandardScaler()\n",
        "X_res_scaled = scaler.fit_transform(X_res)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_res_scaled, y_res, test_size=0.2, random_state=42, stratify=y_res)\n",
        "\n",
        "# Train Random Forest (traditional)\n",
        "rf_model = RandomForestClassifier(random_state=42, n_estimators=200)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and metrics\n",
        "y_pred = rf_model.predict(X_test)\n",
        "y_prob = rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"\\nüéØ Random Forest Performance:\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
        "\n",
        "# SHAP (TreeExplainer) - global + local\n",
        "print(\"\\nüîç SHAP Analysis for Random Forest...\")\n",
        "explainer_rf = shap.TreeExplainer(rf_model)\n",
        "shap_values_rf = explainer_rf.shap_values(X_test)\n",
        "\n",
        "# Summary plot for class 1\n",
        "shap.summary_plot(shap_values_rf[1], X_test, feature_names=selected_features, show=False)\n",
        "plt.title(\"SHAP Feature Importance - Random Forest (selected features)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Local force plot for first instance (matplotlib)\n",
        "try:\n",
        "    shap.force_plot(explainer_rf.expected_value[1], shap_values_rf[1][0,:], X_test[0,:],\n",
        "                    feature_names=selected_features, matplotlib=True)\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(\"SHAP force_plot may not render in this environment:\", e)\n",
        "\n",
        "# LIME (local explanation) - textual + notebook display\n",
        "print(\"\\nüí° LIME Explanation for Random Forest (test instance 0):\")\n",
        "lime_explainer = LimeTabularExplainer(X_train, feature_names=selected_features,\n",
        "                                      class_names=['No CKD','CKD'], mode='classification')\n",
        "lime_exp = lime_explainer.explain_instance(X_test[0], rf_model.predict_proba, num_features=10)\n",
        "print(lime_exp.as_list())\n",
        "try:\n",
        "    lime_exp.show_in_notebook(show_table=True)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# End of Random Forest block\n"
      ],
      "metadata": {
        "id": "6-x78N1Y8tOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Block 2 ‚Äî Stacking Ensemble (uses RF selector pipeline to pick top 12)"
      ],
      "metadata": {
        "id": "WLKelmRd81wQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# Stacking Ensemble (RF selector pipeline -> stacking)\n",
        "# ------------------------------\n",
        "import shap\n",
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# target column name\n",
        "target_col = \"classification\"  # change if needed\n",
        "\n",
        "# Prepare full feature set and labels\n",
        "X_all = df.drop(columns=[target_col])\n",
        "y_all = df[target_col].astype(int)\n",
        "\n",
        "# SMOTE on full feature set (so selector sees balanced data)\n",
        "smote = SMOTE(random_state=42)\n",
        "X_res, y_res = smote.fit_resample(X_all, y_all)\n",
        "\n",
        "# Scale\n",
        "scaler = StandardScaler()\n",
        "X_res_scaled = scaler.fit_transform(X_res)\n",
        "\n",
        "# RandomForest-based feature selection (Select top 12)\n",
        "rf_selector = RandomForestClassifier(random_state=42, n_estimators=200)\n",
        "rf_selector.fit(X_res_scaled, y_res)\n",
        "importances = rf_selector.feature_importances_\n",
        "indices = np.argsort(importances)[::-1][:12]\n",
        "selected_features = X_all.columns[indices].tolist()\n",
        "print(\"Selected features (RF selector):\", selected_features)\n",
        "\n",
        "# Create dataset with selected features\n",
        "X_selected = X_res[selected_features]\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y_res, test_size=0.2, random_state=42, stratify=y_res)\n",
        "\n",
        "# Scale after split\n",
        "scaler2 = StandardScaler()\n",
        "X_train_scaled = scaler2.fit_transform(X_train)\n",
        "X_test_scaled = scaler2.transform(X_test)\n",
        "\n",
        "# Define stacking ensemble (LR + MLP + GB -> LR)\n",
        "base_estimators = [\n",
        "    ('lr', LogisticRegression(max_iter=2000)),\n",
        "    ('mlp', MLPClassifier(max_iter=1000)),\n",
        "    ('gb', GradientBoostingClassifier())\n",
        "]\n",
        "stack_model = StackingClassifier(estimators=base_estimators, final_estimator=LogisticRegression(), cv=5, n_jobs=-1)\n",
        "\n",
        "# Train\n",
        "stack_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict & evaluate\n",
        "y_pred = stack_model.predict(X_test_scaled)\n",
        "try:\n",
        "    y_prob = stack_model.predict_proba(X_test_scaled)[:, 1]\n",
        "except Exception:\n",
        "    # fallback if predict_proba not supported\n",
        "    y_prob = np.zeros(len(y_test))\n",
        "\n",
        "print(\"\\nüéØ Stacking Ensemble Performance:\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "if y_prob.sum() > 0:\n",
        "    print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
        "\n",
        "# SHAP (model-agnostic)\n",
        "print(\"\\nüîç SHAP Analysis for Stacking Ensemble...\")\n",
        "explainer_stack = shap.Explainer(stack_model, X_train_scaled)\n",
        "shap_values_stack = explainer_stack(X_test_scaled)\n",
        "\n",
        "shap.summary_plot(shap_values_stack, X_test_scaled, feature_names=selected_features, show=False)\n",
        "plt.title(\"SHAP Summary - Stacking Ensemble\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# LIME local explanation for an instance\n",
        "print(\"\\nüí° LIME Explanation for Stacking Ensemble (test instance 1):\")\n",
        "lime_explainer = LimeTabularExplainer(X_train_scaled, feature_names=selected_features, class_names=['No CKD','CKD'], mode='classification')\n",
        "lime_exp = lime_explainer.explain_instance(X_test_scaled[1], stack_model.predict_proba, num_features=10)\n",
        "print(lime_exp.as_list())\n",
        "try:\n",
        "    lime_exp.show_in_notebook(show_table=True)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# End of Stacking block\n"
      ],
      "metadata": {
        "id": "JCPkF21U870R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Block 3 ‚Äî Deep NN Bottleneck (exact create_deep_nn_3 architecture; uses RF selector pipeline)"
      ],
      "metadata": {
        "id": "eMUvsDwQ89If"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# Deep NN Bottleneck (create_deep_nn_3) ‚Äî with RF selector pipeline\n",
        "# ------------------------------\n",
        "import shap\n",
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# target column\n",
        "target_col = \"classification\"  # change if needed\n",
        "\n",
        "# Preprocess: SMOTE on full features -> scaling -> RF selector -> use selected features\n",
        "X_all = df.drop(columns=[target_col])\n",
        "y_all = df[target_col].astype(int)\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_res, y_res = smote.fit_resample(X_all, y_all)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_res_scaled = scaler.fit_transform(X_res)\n",
        "\n",
        "# RF selector\n",
        "rf_selector = RandomForestClassifier(random_state=42, n_estimators=200)\n",
        "rf_selector.fit(X_res_scaled, y_res)\n",
        "importances = rf_selector.feature_importances_\n",
        "indices = np.argsort(importances)[::-1][:12]\n",
        "selected_features = X_all.columns[indices].tolist()\n",
        "print(\"Selected features (RF selector):\", selected_features)\n",
        "\n",
        "# Build dataset with selected features\n",
        "X_selected = X_res[selected_features]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y_res, test_size=0.2, random_state=42, stratify=y_res)\n",
        "\n",
        "# Scale after split\n",
        "scaler2 = StandardScaler()\n",
        "X_train_scaled = scaler2.fit_transform(X_train)\n",
        "X_test_scaled = scaler2.transform(X_test)\n",
        "\n",
        "# categorical labels for DNN\n",
        "y_train_cat = to_categorical(y_train)\n",
        "y_test_cat = to_categorical(y_test)\n",
        "\n",
        "# Define create_deep_nn_3 (exact architecture)\n",
        "def create_deep_nn_3(input_dim):\n",
        "    model = Sequential([\n",
        "        Dense(512, activation='relu', input_shape=(input_dim,), kernel_regularizer=l2(0.001)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "\n",
        "        Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.4),\n",
        "\n",
        "        Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Dense(64, activation='relu'),  # Bottleneck\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Dense(128, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Dense(256, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.4),\n",
        "\n",
        "        Dense(2, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Create, compile and train\n",
        "input_dim = X_train_scaled.shape[1]\n",
        "model = create_deep_nn_3(input_dim)\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7, verbose=1),\n",
        "    ModelCheckpoint('best_deep_bottleneck.h5', monitor='val_accuracy', save_best_only=True, verbose=0)\n",
        "]\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.0005), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train_scaled, y_train_cat, validation_data=(X_test_scaled, y_test_cat),\n",
        "                    epochs=150, batch_size=32, callbacks=callbacks, verbose=1)\n",
        "\n",
        "# Evaluate\n",
        "y_prob = model.predict(X_test_scaled)[:, 1]\n",
        "y_pred = np.argmax(model.predict(X_test_scaled), axis=1)\n",
        "\n",
        "print(\"\\nüéØ Deep_NN_Bottleneck Performance:\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
        "\n",
        "# SHAP DeepExplainer\n",
        "print(\"\\nüîç SHAP DeepExplainer for Deep_NN_Bottleneck...\")\n",
        "bg_idx = np.random.choice(X_train_scaled.shape[0], min(100, X_train_scaled.shape[0]), replace=False)\n",
        "background = X_train_scaled[bg_idx]\n",
        "explainer_dnn = shap.DeepExplainer(model, background)\n",
        "shap_values = explainer_dnn.shap_values(X_test_scaled[:100])\n",
        "\n",
        "shap.summary_plot(shap_values[0], X_test_scaled[:100], feature_names=selected_features, show=False)\n",
        "plt.title(\"SHAP Summary - Deep_NN_Bottleneck\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# LIME local explanation (text + notebook)\n",
        "print(\"\\nüí° LIME Explanation for Deep_NN_Bottleneck (test instance 2):\")\n",
        "lime_explainer = LimeTabularExplainer(X_train_scaled, feature_names=selected_features, class_names=['No CKD','CKD'], mode='classification')\n",
        "def model_predict_proba_for_lime(x):\n",
        "    return model.predict(x)  # returns (n,2)\n",
        "lime_exp = lime_explainer.explain_instance(X_test_scaled[2], model_predict_proba_for_lime, num_features=10)\n",
        "print(lime_exp.as_list())\n",
        "try:\n",
        "    lime_exp.show_in_notebook(show_table=True)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# End of Bottleneck block\n"
      ],
      "metadata": {
        "id": "QZTr8g8i8-td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Block 4 ‚Äî Enhanced Bottleneck (Bottleneck Longer; exact longer architecture + long training)"
      ],
      "metadata": {
        "id": "s6S7teL18_kK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# Enhanced Bottleneck (Bottleneck Longer) ‚Äî exact architecture and training config\n",
        "# ------------------------------\n",
        "import shap\n",
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# target column\n",
        "target_col = \"classification\"  # change if needed\n",
        "\n",
        "# Preprocess exactly as Bottleneck block: SMOTE -> scale -> RF selector -> use selected features\n",
        "X_all = df.drop(columns=[target_col])\n",
        "y_all = df[target_col].astype(int)\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_res, y_res = smote.fit_resample(X_all, y_all)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_res_scaled = scaler.fit_transform(X_res)\n",
        "\n",
        "# RF selector\n",
        "rf_selector = RandomForestClassifier(random_state=42, n_estimators=200)\n",
        "rf_selector.fit(X_res_scaled, y_res)\n",
        "importances = rf_selector.feature_importances_\n",
        "indices = np.argsort(importances)[::-1][:12]\n",
        "selected_features = X_all.columns[indices].tolist()\n",
        "print(\"Selected features (RF selector):\", selected_features)\n",
        "\n",
        "# Build selected-feature dataset\n",
        "X_selected = X_res[selected_features]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y_res, test_size=0.2, random_state=42, stratify=y_res)\n",
        "\n",
        "# Scale after split\n",
        "scaler2 = StandardScaler()\n",
        "X_train_scaled = scaler2.fit_transform(X_train)\n",
        "X_test_scaled = scaler2.transform(X_test)\n",
        "\n",
        "y_train_cat = to_categorical(y_train)\n",
        "y_test_cat = to_categorical(y_test)\n",
        "\n",
        "# Define enhanced bottleneck (exact provided architecture)\n",
        "def create_enhanced_bottleneck_early_stop(input_dim):\n",
        "    model = Sequential([\n",
        "        Dense(512, activation='relu', input_shape=(input_dim,), kernel_regularizer=l2(0.001)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "\n",
        "        Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.4),\n",
        "\n",
        "        Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Dense(64, activation='relu'),  # Bottleneck\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Dense(128, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Dense(256, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.4),\n",
        "\n",
        "        Dense(2, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Create model\n",
        "input_dim = X_train_scaled.shape[1]\n",
        "bottleneck_long = create_enhanced_bottleneck_early_stop(input_dim)\n",
        "\n",
        "# Callbacks & compile (long training configuration)\n",
        "callbacks_long = [\n",
        "    EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True, verbose=1, min_delta=1e-4),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=20, min_lr=1e-7, verbose=1),\n",
        "    ModelCheckpoint('best_bottleneck_long.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "]\n",
        "\n",
        "bottleneck_long.compile(optimizer=Adam(learning_rate=0.0005), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train (may stop earlier due to early stopping)\n",
        "history = bottleneck_long.fit(X_train_scaled, y_train_cat, validation_data=(X_test_scaled, y_test_cat),\n",
        "                              epochs=500, batch_size=32, callbacks=callbacks_long, verbose=1)\n",
        "\n",
        "# Evaluate\n",
        "y_prob = bottleneck_long.predict(X_test_scaled)[:, 1]\n",
        "y_pred = np.argmax(bottleneck_long.predict(X_test_scaled), axis=1)\n",
        "\n",
        "print(\"\\nüéØ Enhanced Bottleneck (Long) Performance:\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
        "\n",
        "# SHAP DeepExplainer\n",
        "print(\"\\nüîç SHAP DeepExplainer for Bottleneck Longer...\")\n",
        "bg_idx = np.random.choice(X_train_scaled.shape[0], min(100, X_train_scaled.shape[0]), replace=False)\n",
        "background = X_train_scaled[bg_idx]\n",
        "explainer_long = shap.DeepExplainer(bottleneck_long, background)\n",
        "shap_values = explainer_long.shap_values(X_test_scaled[:100])\n",
        "\n",
        "shap.summary_plot(shap_values[0], X_test_scaled[:100], feature_names=selected_features, show=False)\n",
        "plt.title(\"SHAP Summary - Bottleneck Longer\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# LIME for local explanation\n",
        "print(\"\\nüí° LIME Explanation for Bottleneck Longer (test instance 3):\")\n",
        "lime_explainer = LimeTabularExplainer(X_train_scaled, feature_names=selected_features, class_names=['No CKD','CKD'], mode='classification')\n",
        "def bottleneck_long_predict_proba(x):\n",
        "    return bottleneck_long.predict(x)\n",
        "lime_exp = lime_explainer.explain_instance(X_test_scaled[3], bottleneck_long_predict_proba, num_features=10)\n",
        "print(lime_exp.as_list())\n",
        "try:\n",
        "    lime_exp.show_in_notebook(show_table=True)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# End of Enhanced Bottleneck block\n"
      ],
      "metadata": {
        "id": "dCzKG2JG9BTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- DEEP SEEK"
      ],
      "metadata": {
        "id": "LTUPD0Rd8oyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# READY-TO-RUN SHAP + LIME FOR TOP 4 MODELS\n",
        "# Maintains Exact Same Architecture & RF Selector Pipeline\n",
        "# =============================================================================\n",
        "\n",
        "import shap\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.inspection import partial_dependence\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"SHAP version:\", shap.__version__)\n",
        "print(\"LIME available\")\n",
        "\n",
        "class Top4ModelsXAI:\n",
        "    \"\"\"\n",
        "    Comprehensive XAI for Top 4 Champion Models\n",
        "    Maintains exact same RF selector pipeline and architecture\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, models_dict, feature_names, X_train, X_test, y_train, y_test):\n",
        "        self.models = models_dict\n",
        "        self.feature_names = feature_names\n",
        "        self.X_train = X_train\n",
        "        self.X_test = X_test\n",
        "        self.y_train = y_train\n",
        "        self.y_test = y_test\n",
        "        self.results = {}\n",
        "\n",
        "        # Set style for professional plots\n",
        "        plt.style.use('default')\n",
        "        sns.set_palette(\"husl\")\n",
        "\n",
        "        print(\"üéØ Top 4 Models Selected for XAI:\")\n",
        "        for model_name in self.models.keys():\n",
        "            print(f\"   ‚Ä¢ {model_name}\")\n",
        "\n",
        "    # =========================================================================\n",
        "    # SHAP IMPLEMENTATION FOR ALL MODEL TYPES\n",
        "    # =========================================================================\n",
        "\n",
        "    def shap_analysis_stacking_ensemble(self, model_name):\n",
        "        \"\"\"SHAP analysis for Stacking Ensemble models\"\"\"\n",
        "        print(f\"\\nüìä SHAP Analysis - {model_name}\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        model = self.models[model_name]\n",
        "\n",
        "        try:\n",
        "            # Sample for efficiency\n",
        "            X_sample = self.X_test[:100]\n",
        "\n",
        "            # Use KernelSHAP for complex ensembles\n",
        "            explainer = shap.KernelExplainer(model.predict_proba, self.X_train[:50])\n",
        "            shap_values = explainer.shap_values(X_sample)\n",
        "\n",
        "            # Handle binary classification\n",
        "            if isinstance(shap_values, list):\n",
        "                shap_values = shap_values[1]  # Use class 1 (CKD)\n",
        "\n",
        "            # Summary plot\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            shap.summary_plot(shap_values, X_sample, feature_names=self.feature_names, show=False)\n",
        "            plt.title(f'SHAP Summary - {model_name}\\n(Impact on CKD Prediction)',\n",
        "                     fontsize=16, fontweight='bold', pad=20)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # Bar plot\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            shap.summary_plot(shap_values, X_sample, feature_names=self.feature_names,\n",
        "                             plot_type=\"bar\", show=False)\n",
        "            plt.title(f'SHAP Feature Importance - {model_name}',\n",
        "                     fontsize=16, fontweight='bold', pad=20)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # Calculate and store importance\n",
        "            mean_abs_shap = np.mean(np.abs(shap_values), axis=0)\n",
        "            importance_df = pd.DataFrame({\n",
        "                'Feature': self.feature_names,\n",
        "                'SHAP_Importance': mean_abs_shap\n",
        "            }).sort_values('SHAP_Importance', ascending=False)\n",
        "\n",
        "            print(\"üîù Top 5 Features:\")\n",
        "            print(importance_df.head().round(4))\n",
        "\n",
        "            self.results[model_name] = {\n",
        "                'shap_values': shap_values,\n",
        "                'importance_df': importance_df,\n",
        "                'explainer': explainer\n",
        "            }\n",
        "\n",
        "            return explainer, shap_values, importance_df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå SHAP failed for {model_name}: {str(e)}\")\n",
        "            return None, None, None\n",
        "\n",
        "    def shap_analysis_random_forest(self, model_name):\n",
        "        \"\"\"SHAP analysis for Random Forest (TreeExplainer)\"\"\"\n",
        "        print(f\"\\nüìä SHAP Analysis - {model_name}\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        model = self.models[model_name]\n",
        "\n",
        "        try:\n",
        "            # Use TreeExplainer for Random Forest\n",
        "            explainer = shap.TreeExplainer(model)\n",
        "            shap_values = explainer.shap_values(self.X_test[:100])\n",
        "\n",
        "            # Handle binary classification\n",
        "            if isinstance(shap_values, list):\n",
        "                shap_values = shap_values[1]  # Use class 1 (CKD)\n",
        "\n",
        "            # Summary plot\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            shap.summary_plot(shap_values, self.X_test[:100], feature_names=self.feature_names, show=False)\n",
        "            plt.title(f'SHAP Summary - {model_name}\\n(Impact on CKD Prediction)',\n",
        "                     fontsize=16, fontweight='bold', pad=20)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # Bar plot\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            shap.summary_plot(shap_values, self.X_test[:100], feature_names=self.feature_names,\n",
        "                             plot_type=\"bar\", show=False)\n",
        "            plt.title(f'SHAP Feature Importance - {model_name}',\n",
        "                     fontsize=16, fontweight='bold', pad=20)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # Force plot for first instance\n",
        "            plt.figure(figsize=(12, 4))\n",
        "            shap.force_plot(explainer.expected_value[1], shap_values[0],\n",
        "                          self.X_test[0], feature_names=self.feature_names, matplotlib=True, show=False)\n",
        "            plt.title(f'SHAP Force Plot - First Instance\\n{model_name}', fontsize=14, fontweight='bold')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # Calculate importance\n",
        "            mean_abs_shap = np.mean(np.abs(shap_values), axis=0)\n",
        "            importance_df = pd.DataFrame({\n",
        "                'Feature': self.feature_names,\n",
        "                'SHAP_Importance': mean_abs_shap\n",
        "            }).sort_values('SHAP_Importance', ascending=False)\n",
        "\n",
        "            print(\"üîù Top 5 Features:\")\n",
        "            print(importance_df.head().round(4))\n",
        "\n",
        "            self.results[model_name] = {\n",
        "                'shap_values': shap_values,\n",
        "                'importance_df': importance_df,\n",
        "                'explainer': explainer\n",
        "            }\n",
        "\n",
        "            return explainer, shap_values, importance_df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå SHAP failed for {model_name}: {str(e)}\")\n",
        "            return None, None, None\n",
        "\n",
        "    def shap_analysis_dnn(self, model_name):\n",
        "        \"\"\"SHAP analysis for Deep Neural Network\"\"\"\n",
        "        print(f\"\\nüß† SHAP Analysis - {model_name}\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        model = self.models[model_name]\n",
        "\n",
        "        try:\n",
        "            # Use DeepExplainer for neural networks\n",
        "            background = self.X_train[np.random.choice(len(self.X_train), 50, replace=False)]\n",
        "            explainer = shap.DeepExplainer(model, background)\n",
        "            shap_values = explainer.shap_values(self.X_test[:50])\n",
        "\n",
        "            # Handle output format\n",
        "            if isinstance(shap_values, list):\n",
        "                shap_values = shap_values[0]  # For DNN with softmax output\n",
        "\n",
        "            # Summary plot\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            shap.summary_plot(shap_values, self.X_test[:50], feature_names=self.feature_names, show=False)\n",
        "            plt.title(f'SHAP Summary - {model_name}\\n(Deep Neural Network)',\n",
        "                     fontsize=16, fontweight='bold', pad=20)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # Bar plot\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            shap.summary_plot(shap_values, self.X_test[:50], feature_names=self.feature_names,\n",
        "                             plot_type=\"bar\", show=False)\n",
        "            plt.title(f'SHAP Feature Importance - {model_name}',\n",
        "                     fontsize=16, fontweight='bold', pad=20)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # Calculate importance\n",
        "            mean_abs_shap = np.mean(np.abs(shap_values), axis=0)\n",
        "            importance_df = pd.DataFrame({\n",
        "                'Feature': self.feature_names,\n",
        "                'SHAP_Importance': mean_abs_shap\n",
        "            }).sort_values('SHAP_Importance', ascending=False)\n",
        "\n",
        "            print(\"üîù Top 5 Features:\")\n",
        "            print(importance_df.head().round(4))\n",
        "\n",
        "            self.results[model_name] = {\n",
        "                'shap_values': shap_values,\n",
        "                'importance_df': importance_df,\n",
        "                'explainer': explainer\n",
        "            }\n",
        "\n",
        "            return explainer, shap_values, importance_df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå SHAP failed for {model_name}: {str(e)}\")\n",
        "            # Fallback to KernelSHAP\n",
        "            return self._shap_fallback_dnn(model, model_name)\n",
        "\n",
        "    def _shap_fallback_dnn(self, model, model_name):\n",
        "        \"\"\"Fallback SHAP method for DNN\"\"\"\n",
        "        try:\n",
        "            def predict_wrapper(x):\n",
        "                return model.predict(x, verbose=0)\n",
        "\n",
        "            explainer = shap.KernelExplainer(predict_wrapper, self.X_train[:50])\n",
        "            shap_values = explainer.shap_values(self.X_test[:50])\n",
        "\n",
        "            if isinstance(shap_values, list):\n",
        "                shap_values = shap_values[0]\n",
        "\n",
        "            # Create plots\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            shap.summary_plot(shap_values, self.X_test[:50], feature_names=self.feature_names, show=False)\n",
        "            plt.title(f'SHAP Summary - {model_name} (KernelSHAP)', fontsize=16, fontweight='bold')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            mean_abs_shap = np.mean(np.abs(shap_values), axis=0)\n",
        "            importance_df = pd.DataFrame({\n",
        "                'Feature': self.feature_names,\n",
        "                'SHAP_Importance': mean_abs_shap\n",
        "            }).sort_values('SHAP_Importance', ascending=False)\n",
        "\n",
        "            return explainer, shap_values, importance_df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå SHAP fallback also failed: {str(e)}\")\n",
        "            return None, None, None\n",
        "\n",
        "    # =========================================================================\n",
        "    # LIME IMPLEMENTATION FOR ALL MODELS\n",
        "    # =========================================================================\n",
        "\n",
        "    def lime_analysis_all_models(self, instance_indices=[0, 1, 2]):\n",
        "        \"\"\"LIME analysis for all models - same instances for comparison\"\"\"\n",
        "        print(f\"\\nüéØ LIME Local Explanations - All Models\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        for model_name, model in self.models.items():\n",
        "            print(f\"\\nüìã LIME Analysis - {model_name}\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "            for instance_idx in instance_indices:\n",
        "                if instance_idx >= len(self.X_test):\n",
        "                    continue\n",
        "\n",
        "                self._lime_single_instance(model, model_name, instance_idx)\n",
        "\n",
        "    def _lime_single_instance(self, model, model_name, instance_idx):\n",
        "        \"\"\"LIME explanation for single instance\"\"\"\n",
        "        try:\n",
        "            # Create LIME explainer\n",
        "            explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "                self.X_train,\n",
        "                feature_names=self.feature_names,\n",
        "                class_names=['No CKD', 'CKD'],\n",
        "                mode='classification',\n",
        "                random_state=42\n",
        "            )\n",
        "\n",
        "            # Explain instance\n",
        "            exp = explainer.explain_instance(\n",
        "                self.X_test[instance_idx],\n",
        "                model.predict_proba,\n",
        "                num_features=len(self.feature_names),\n",
        "                top_labels=1\n",
        "            )\n",
        "\n",
        "            # Get prediction details\n",
        "            actual_label = \"CKD\" if self.y_test[instance_idx] == 1 else \"No CKD\"\n",
        "            prediction_proba = model.predict_proba(self.X_test[instance_idx:instance_idx+1])[0]\n",
        "            predicted_label = \"CKD\" if np.argmax(prediction_proba) == 1 else \"No CKD\"\n",
        "\n",
        "            print(f\"\\nInstance {instance_idx}:\")\n",
        "            print(f\"  Actual: {actual_label}, Predicted: {predicted_label}\")\n",
        "            print(f\"  Confidence: No CKD: {prediction_proba[0]:.3f}, CKD: {prediction_proba[1]:.3f}\")\n",
        "\n",
        "            # Plot explanation\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            exp.as_pyplot_figure()\n",
        "            plt.title(f'LIME - {model_name} (Instance {instance_idx})\\nActual: {actual_label}, Predicted: {predicted_label}',\n",
        "                     fontsize=12, fontweight='bold')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # Print top features\n",
        "            print(\"  Top Feature Contributions:\")\n",
        "            for feature, weight in exp.local_exp[1][:3]:\n",
        "                feature_name = self.feature_names[feature]\n",
        "                print(f\"    {feature_name}: {weight:+.3f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå LIME failed for {model_name} instance {instance_idx}: {str(e)}\")\n",
        "\n",
        "    # =========================================================================\n",
        "    # COMPREHENSIVE XAI PIPELINE\n",
        "    # =========================================================================\n",
        "\n",
        "    def run_comprehensive_xai(self):\n",
        "        \"\"\"Run complete XAI pipeline for all top models\"\"\"\n",
        "        print(\"\\n\" + \"üöÄ\" * 20)\n",
        "        print(\"COMPREHENSIVE XAI PIPELINE - TOP 4 MODELS\")\n",
        "        print(\"üöÄ\" * 20)\n",
        "\n",
        "        # SHAP Analysis for each model type\n",
        "        shap_results = {}\n",
        "\n",
        "        for model_name in self.models.keys():\n",
        "            if 'Stacking' in model_name:\n",
        "                shap_results[model_name] = self.shap_analysis_stacking_ensemble(model_name)\n",
        "            elif 'Random' in model_name:\n",
        "                shap_results[model_name] = self.shap_analysis_random_forest(model_name)\n",
        "            elif 'DNN' in model_name or 'Bottleneck' in model_name:\n",
        "                shap_results[model_name] = self.shap_analysis_dnn(model_name)\n",
        "            else:\n",
        "                shap_results[model_name] = self.shap_analysis_stacking_ensemble(model_name)  # Default\n",
        "\n",
        "        # LIME Analysis\n",
        "        self.lime_analysis_all_models(instance_indices=[0, 1, 2])\n",
        "\n",
        "        # Comparative Analysis\n",
        "        self._create_comparative_analysis()\n",
        "\n",
        "        # Clinical Insights\n",
        "        self._generate_clinical_insights()\n",
        "\n",
        "        print(f\"\\n‚úÖ XAI PIPELINE COMPLETED!\")\n",
        "        return self.results\n",
        "\n",
        "    def _create_comparative_analysis(self):\n",
        "        \"\"\"Create comparative analysis across all models\"\"\"\n",
        "        print(f\"\\nüìà COMPARATIVE FEATURE IMPORTANCE ANALYSIS\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Collect importance data\n",
        "        comparison_data = []\n",
        "\n",
        "        for model_name in self.models.keys():\n",
        "            if model_name in self.results and 'importance_df' in self.results[model_name]:\n",
        "                importance_df = self.results[model_name]['importance_df']\n",
        "                for _, row in importance_df.iterrows():\n",
        "                    comparison_data.append({\n",
        "                        'Model': model_name,\n",
        "                        'Feature': row['Feature'],\n",
        "                        'Importance': row['SHAP_Importance']\n",
        "                    })\n",
        "\n",
        "        if comparison_data:\n",
        "            comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "            # Create comparison heatmap\n",
        "            pivot_df = comparison_df.pivot(index='Feature', columns='Model', values='Importance')\n",
        "\n",
        "            plt.figure(figsize=(14, 10))\n",
        "            sns.heatmap(pivot_df, annot=True, cmap='YlOrRd', fmt='.3f',\n",
        "                       cbar_kws={'label': 'SHAP Importance'})\n",
        "            plt.title('Comparative Feature Importance - Top 4 Models\\n(SHAP Mean |Impact|)',\n",
        "                     fontsize=16, fontweight='bold', pad=20)\n",
        "            plt.xticks(rotation=45, ha='right')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # Top features consensus\n",
        "            print(\"\\nüîù CONSENSUS TOP FEATURES Across All Models:\")\n",
        "            feature_consensus = pivot_df.mean(axis=1).sort_values(ascending=False)\n",
        "            print(feature_consensus.head(8).round(4))\n",
        "\n",
        "    def _generate_clinical_insights(self):\n",
        "        \"\"\"Generate clinical insights from XAI results\"\"\"\n",
        "        print(f\"\\nüè• CLINICAL INSIGHTS FROM XAI ANALYSIS\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        insights = {\n",
        "            'Key Biomarkers': [\n",
        "                \"ü©∏ **Albumin levels** - Strongest predictor across all models\",\n",
        "                \"üíâ **Blood urea** - Consistent high importance for kidney function\",\n",
        "                \"ü©∫ **Hemoglobin** - Critical for anemia detection in CKD\",\n",
        "                \"‚öñÔ∏è **Specific gravity** - Kidney concentration ability\"\n",
        "            ],\n",
        "            'Model-Specific Insights': [\n",
        "                \"üéØ **Stacking Ensemble** - Leverages diverse model strengths\",\n",
        "                \"üå≥ **Random Forest** - Provides stable, interpretable feature rankings\",\n",
        "                \"üß† **DNN Bottleneck** - Captures complex non-linear relationships\",\n",
        "                \"üöÄ **Enhanced Stacking** - Highest accuracy with good interpretability\"\n",
        "            ],\n",
        "            'Clinical Recommendations': [\n",
        "                \"üìä **Focus on top biomarkers** for routine monitoring\",\n",
        "                \"üéöÔ∏è **Use model confidence** for clinical decision thresholds\",\n",
        "                \"üîç **Validate feature importance** with medical expertise\",\n",
        "                \"üìà **Monitor trends** in key features over time\"\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        for category, items in insights.items():\n",
        "            print(f\"\\n{category}:\")\n",
        "            for item in items:\n",
        "                print(f\"  {item}\")\n",
        "\n",
        "# =============================================================================\n",
        "# READY-TO-RUN INTEGRATION WITH YOUR EXISTING CODE\n",
        "# =============================================================================\n",
        "\n",
        "def setup_top_4_models_xai():\n",
        "    \"\"\"\n",
        "    Setup XAI for Top 4 models using your exact architecture\n",
        "    \"\"\"\n",
        "\n",
        "    # Your feature names from RF Selector (12 features)\n",
        "    feature_names = [\n",
        "        'blood pressure', 'specific gravity', 'albumin', 'sugar',\n",
        "        'blood glucose random', 'blood urea', 'sodium', 'potassium',\n",
        "        'hemoglobin', 'packed cell volume', 'white blood cell count',\n",
        "        'red blood cell count'\n",
        "    ]\n",
        "\n",
        "    print(\"üîß Setting up XAI for Top 4 Models...\")\n",
        "    print(f\"üìã Using {len(feature_names)} features from RF Selector\")\n",
        "    print(\"Features:\", feature_names)\n",
        "\n",
        "    # Initialize models (you'll replace these with your actual trained models)\n",
        "    top_4_models = {\n",
        "        'Stacking_Ensemble': None,  # Replace with your stacking_model\n",
        "        'Random_Forest': None,      # Replace with your rf_model\n",
        "        'Deep_NN_Bottleneck': None, # Replace with your bottleneck_model\n",
        "        'Enhanced_Stacking': None   # Replace with your enhanced_stacking_model\n",
        "    }\n",
        "\n",
        "    # Remove None models (placeholder for your actual models)\n",
        "    top_4_models = {k: v for k, v in top_4_models.items() if v is not None}\n",
        "\n",
        "    if not top_4_models:\n",
        "        print(\"‚ùå Please load your trained models first\")\n",
        "        return None\n",
        "\n",
        "    # Initialize XAI engine\n",
        "    xai_engine = Top4ModelsXAI(\n",
        "        models_dict=top_4_models,\n",
        "        feature_names=feature_names,\n",
        "        X_train=X_train,  # From your ensemble code\n",
        "        X_test=X_test,    # From your ensemble code\n",
        "        y_train=y_train,  # From your ensemble code\n",
        "        y_test=y_test     # From your ensemble code\n",
        "    )\n",
        "\n",
        "    return xai_engine\n",
        "\n",
        "# =============================================================================\n",
        "# QUICK START - READY TO RUN\n",
        "# =============================================================================\n",
        "\n",
        "def quick_start_xai():\n",
        "    \"\"\"\n",
        "    One-function call to run complete XAI analysis\n",
        "    \"\"\"\n",
        "    print(\"üöÄ QUICK START: Top 4 Models XAI Analysis\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Setup XAI engine\n",
        "    xai_engine = setup_top_4_models_xai()\n",
        "\n",
        "    if xai_engine is None:\n",
        "        print(\"‚ùå Could not initialize XAI engine. Please check your models.\")\n",
        "        return\n",
        "\n",
        "    # Run comprehensive analysis\n",
        "    results = xai_engine.run_comprehensive_xai()\n",
        "\n",
        "    print(f\"\\nüéâ XAI ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
        "    print(f\"üìä Analyzed {len(xai_engine.models)} champion models\")\n",
        "    print(f\"üéØ Generated: SHAP global importance + LIME local explanations\")\n",
        "    print(f\"üè• Delivered: Clinical insights and comparative analysis\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# =============================================================================\n",
        "# MODEL-SPECIFIC INTEGRATION HELPERS\n",
        "# =============================================================================\n",
        "\n",
        "def integrate_with_your_models():\n",
        "    \"\"\"\n",
        "    How to integrate with your existing trained models\n",
        "    \"\"\"\n",
        "\n",
        "    # AFTER running your training code, collect the models like this:\n",
        "\n",
        "    # From Traditional ML code:\n",
        "    # models['Random Forest'] is your Random Forest model\n",
        "\n",
        "    # From Ensemble code:\n",
        "    # original_stacking, enhanced_stacking are your ensemble models\n",
        "\n",
        "    # From DNN code:\n",
        "    # bottleneck_model is your Deep NN Bottleneck model\n",
        "\n",
        "    top_4_models = {\n",
        "        'Stacking_Ensemble': original_stacking,      # From ensemble code\n",
        "        'Random_Forest': models['Random Forest'],    # From traditional ML code\n",
        "        'Deep_NN_Bottleneck': bottleneck_model,      # From DNN code\n",
        "        'Enhanced_Stacking': enhanced_stacking       # From ensemble code\n",
        "    }\n",
        "\n",
        "    # Make sure models are trained\n",
        "    for name, model in top_4_models.items():\n",
        "        if model is None:\n",
        "            print(f\"‚ö†Ô∏è {name} model is None - please train it first\")\n",
        "        elif not hasattr(model, 'predict_proba'):\n",
        "            print(f\"‚ö†Ô∏è {name} doesn't have predict_proba method\")\n",
        "\n",
        "    return top_4_models\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN EXECUTION\n",
        "# =============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\"\n",
        "    RUN THIS CODE AFTER YOUR MODELS ARE TRAINED\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"üéØ CHRONIC KIDNEY DISEASE - TOP 4 MODELS XAI\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Option 1: Quick start (uncomment when models are ready)\n",
        "    # results = quick_start_xai()\n",
        "\n",
        "    # Option 2: Manual setup\n",
        "    print(\"\\nüìù Manual Setup Instructions:\")\n",
        "    print(\"1. Train all your models first\")\n",
        "    print(\"2. Replace None values in top_4_models with your actual models\")\n",
        "    print(\"3. Call quick_start_xai() or run the XAI pipeline manually\")\n",
        "    print(\"4. Ensure X_train, X_test, y_train, y_test are available from ensemble code\")\n",
        "\n",
        "    print(\"\\nüîß Required Models:\")\n",
        "    print(\"   ‚Ä¢ Stacking_Ensemble (original_stacking from ensemble code)\")\n",
        "    print(\"   ‚Ä¢ Random_Forest (from traditional ML code)\")\n",
        "    print(\"   ‚Ä¢ Deep_NN_Bottleneck (from DNN code)\")\n",
        "    print(\"   ‚Ä¢ Enhanced_Stacking (enhanced_stacking from ensemble code)\")\n",
        "\n",
        "    print(\"\\n‚úÖ Code is ready to run once models are trained!\")"
      ],
      "metadata": {
        "id": "XxvYbfgp4fo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üéØ 1. Stacking Ensemble XAI (Best Overall)"
      ],
      "metadata": {
        "id": "y2F3evd97uRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# STACKING ENSEMBLE XAI - Maintains Exact RF Selector Pipeline\n",
        "# =============================================================================\n",
        "\n",
        "import shap\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.inspection import partial_dependence\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class StackingEnsembleXAI:\n",
        "    \"\"\"\n",
        "    XAI for Stacking Ensemble - Maintains exact RF selector pipeline from ensemble code\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, stacking_model, X_train, X_test, y_train, y_test):\n",
        "        self.model = stacking_model\n",
        "        self.X_train = X_train  # Already processed through RF selector\n",
        "        self.X_test = X_test    # Already processed through RF selector\n",
        "        self.y_train = y_train\n",
        "        self.y_test = y_test\n",
        "\n",
        "        # Feature names from RF selector (12 features)\n",
        "        self.feature_names = [\n",
        "            'blood pressure', 'specific gravity', 'albumin', 'sugar',\n",
        "            'blood glucose random', 'blood urea', 'sodium', 'potassium',\n",
        "            'hemoglobin', 'packed cell volume', 'white blood cell count',\n",
        "            'red blood cell count'\n",
        "        ]\n",
        "\n",
        "        self.results = {}\n",
        "\n",
        "        print(\"üéØ Stacking Ensemble XAI Initialized\")\n",
        "        print(f\"üìä Using {len(self.feature_names)} RF-selected features\")\n",
        "        print(f\"üèóÔ∏è Model: {type(self.model).__name__}\")\n",
        "\n",
        "    def shap_analysis_stacking(self):\n",
        "        \"\"\"SHAP analysis for Stacking Ensemble\"\"\"\n",
        "        print(\"\\nüìä SHAP Analysis - Stacking Ensemble\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        try:\n",
        "            # Sample for efficiency (using same approach as ensemble code)\n",
        "            X_sample = self.X_test[:100]\n",
        "\n",
        "            # KernelSHAP for complex ensembles\n",
        "            explainer = shap.KernelExplainer(self.model.predict_proba, self.X_train[:50])\n",
        "            shap_values = explainer.shap_values(X_sample)\n",
        "\n",
        "            # Handle binary classification output\n",
        "            if isinstance(shap_values, list):\n",
        "                shap_values = shap_values[1]  # Use class 1 (CKD)\n",
        "\n",
        "            # 1. Summary Plot\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            shap.summary_plot(shap_values, X_sample, feature_names=self.feature_names, show=False)\n",
        "            plt.title('SHAP Summary - Stacking Ensemble\\n(Impact on CKD Prediction)',\n",
        "                     fontsize=16, fontweight='bold', pad=20)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # 2. Bar Plot\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            shap.summary_plot(shap_values, X_sample, feature_names=self.feature_names,\n",
        "                             plot_type=\"bar\", show=False)\n",
        "            plt.title('SHAP Feature Importance - Stacking Ensemble',\n",
        "                     fontsize=16, fontweight='bold', pad=20)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # 3. Force Plot for first instance\n",
        "            plt.figure(figsize=(12, 4))\n",
        "            shap.force_plot(explainer.expected_value[1], shap_values[0],\n",
        "                          X_sample[0], feature_names=self.feature_names, matplotlib=True, show=False)\n",
        "            plt.title('SHAP Force Plot - First Instance\\nStacking Ensemble',\n",
        "                     fontsize=14, fontweight='bold')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # Calculate feature importance\n",
        "            mean_abs_shap = np.mean(np.abs(shap_values), axis=0)\n",
        "            importance_df = pd.DataFrame({\n",
        "                'Feature': self.feature_names,\n",
        "                'SHAP_Importance': mean_abs_shap\n",
        "            }).sort_values('SHAP_Importance', ascending=False)\n",
        "\n",
        "            print(\"üîù Top Features (Stacking Ensemble):\")\n",
        "            print(importance_df.head(10).round(4))\n",
        "\n",
        "            self.results['shap'] = {\n",
        "                'explainer': explainer,\n",
        "                'shap_values': shap_values,\n",
        "                'importance_df': importance_df\n",
        "            }\n",
        "\n",
        "            return explainer, shap_values, importance_df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå SHAP failed for Stacking Ensemble: {str(e)}\")\n",
        "            return None, None, None\n",
        "\n",
        "    def lime_analysis_stacking(self, instance_indices=[0, 1, 2]):\n",
        "        \"\"\"LIME analysis for Stacking Ensemble\"\"\"\n",
        "        print(f\"\\nüéØ LIME Local Explanations - Stacking Ensemble\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        try:\n",
        "            # Create LIME explainer\n",
        "            explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "                self.X_train,\n",
        "                feature_names=self.feature_names,\n",
        "                class_names=['No CKD', 'CKD'],\n",
        "                mode='classification',\n",
        "                random_state=42,\n",
        "                verbose=False\n",
        "            )\n",
        "\n",
        "            for instance_idx in instance_indices:\n",
        "                if instance_idx >= len(self.X_test):\n",
        "                    continue\n",
        "\n",
        "                # Explain instance\n",
        "                exp = explainer.explain_instance(\n",
        "                    self.X_test[instance_idx],\n",
        "                    self.model.predict_proba,\n",
        "                    num_features=len(self.feature_names),\n",
        "                    top_labels=1\n",
        "                )\n",
        "\n",
        "                # Get prediction details\n",
        "                actual_label = \"CKD\" if self.y_test[instance_idx] == 1 else \"No CKD\"\n",
        "                prediction_proba = self.model.predict_proba(self.X_test[instance_idx:instance_idx+1])[0]\n",
        "                predicted_label = \"CKD\" if np.argmax(prediction_proba) == 1 else \"No CKD\"\n",
        "\n",
        "                print(f\"\\nüìã Instance {instance_idx}:\")\n",
        "                print(f\"   Actual: {actual_label}, Predicted: {predicted_label}\")\n",
        "                print(f\"   Confidence: No CKD: {prediction_proba[0]:.3f}, CKD: {prediction_proba[1]:.3f}\")\n",
        "\n",
        "                # Plot explanation\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                exp.as_pyplot_figure()\n",
        "                plt.title(f'LIME - Stacking Ensemble (Instance {instance_idx})\\nActual: {actual_label}, Predicted: {predicted_label}',\n",
        "                         fontsize=12, fontweight='bold')\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "                # Print top features\n",
        "                print(\"   Top Feature Contributions:\")\n",
        "                for feature, weight in exp.local_exp[1][:5]:\n",
        "                    feature_name = self.feature_names[feature]\n",
        "                    direction = \"increases\" if weight > 0 else \"decreases\"\n",
        "                    print(f\"     {feature_name}: {weight:+.3f} ({direction} CKD risk)\")\n",
        "\n",
        "            self.results['lime'] = explainer\n",
        "            return explainer\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå LIME failed for Stacking Ensemble: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def partial_dependence_analysis(self):\n",
        "        \"\"\"Partial Dependence Plots for Stacking Ensemble\"\"\"\n",
        "        print(f\"\\nüìà Partial Dependence Analysis - Stacking Ensemble\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        try:\n",
        "            # Get top 6 features from SHAP\n",
        "            if 'shap' in self.results:\n",
        "                top_features = self.results['shap']['importance_df'].head(6)['Feature'].tolist()\n",
        "            else:\n",
        "                # Default top features\n",
        "                top_features = ['albumin', 'blood urea', 'hemoglobin', 'specific gravity',\n",
        "                              'blood glucose random', 'blood pressure']\n",
        "\n",
        "            feature_indices = [self.feature_names.index(f) for f in top_features\n",
        "                             if f in self.feature_names]\n",
        "\n",
        "            # Create PDP plots\n",
        "            fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "            axes = axes.ravel()\n",
        "\n",
        "            for i, feature_idx in enumerate(feature_indices):\n",
        "                if i >= len(axes):\n",
        "                    break\n",
        "\n",
        "                # Calculate partial dependence manually\n",
        "                feature_range = np.linspace(\n",
        "                    np.percentile(self.X_train[:, feature_idx], 5),\n",
        "                    np.percentile(self.X_train[:, feature_idx], 95),\n",
        "                    20\n",
        "                )\n",
        "\n",
        "                pdp_values = []\n",
        "                for value in feature_range:\n",
        "                    X_temp = self.X_test.copy()\n",
        "                    X_temp[:, feature_idx] = value\n",
        "                    predictions = self.model.predict_proba(X_temp)[:, 1]\n",
        "                    pdp_values.append(np.mean(predictions))\n",
        "\n",
        "                # Plot\n",
        "                axes[i].plot(feature_range, pdp_values, linewidth=3, color='blue', alpha=0.8)\n",
        "                axes[i].set_xlabel(self.feature_names[feature_idx], fontweight='bold')\n",
        "                axes[i].set_ylabel('Predicted Probability of CKD')\n",
        "                axes[i].set_title(f'PDP: {self.feature_names[feature_idx]}', fontweight='bold')\n",
        "                axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "                # Add histogram\n",
        "                ax2 = axes[i].twinx()\n",
        "                ax2.hist(self.X_train[:, feature_idx], bins=20, alpha=0.3, color='red', density=True)\n",
        "                ax2.set_ylabel('Density', color='red')\n",
        "                ax2.tick_params(axis='y', labelcolor='red')\n",
        "\n",
        "            # Remove empty subplots\n",
        "            for i in range(len(feature_indices), len(axes)):\n",
        "                fig.delaxes(axes[i])\n",
        "\n",
        "            plt.suptitle('Partial Dependence Plots - Stacking Ensemble\\n(How features affect CKD probability)',\n",
        "                        fontsize=16, fontweight='bold', y=1.02)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Partial Dependence failed: {str(e)}\")\n",
        "\n",
        "    def run_complete_xai(self):\n",
        "        \"\"\"Run complete XAI pipeline for Stacking Ensemble\"\"\"\n",
        "        print(\"\\n\" + \"üöÄ\" * 20)\n",
        "        print(\"STACKING ENSEMBLE COMPLETE XAI ANALYSIS\")\n",
        "        print(\"üöÄ\" * 20)\n",
        "\n",
        "        # 1. SHAP Analysis\n",
        "        self.shap_analysis_stacking()\n",
        "\n",
        "        # 2. LIME Analysis\n",
        "        self.lime_analysis_stacking()\n",
        "\n",
        "        # 3. Partial Dependence\n",
        "        self.partial_dependence_analysis()\n",
        "\n",
        "        # 4. Generate insights\n",
        "        self._generate_stacking_insights()\n",
        "\n",
        "        print(f\"\\n‚úÖ STACKING ENSEMBLE XAI COMPLETED!\")\n",
        "        return self.results\n",
        "\n",
        "    def _generate_stacking_insights(self):\n",
        "        \"\"\"Generate specific insights for Stacking Ensemble\"\"\"\n",
        "        print(f\"\\nüè• STACKING ENSEMBLE CLINICAL INSIGHTS\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        insights = [\n",
        "            \"üéØ **Ensemble Strength** - Combines multiple models for robust predictions\",\n",
        "            \"üìä **Feature Consensus** - Uses agreement across base models\",\n",
        "            \"üõ°Ô∏è **Error Reduction** - Less prone to individual model biases\",\n",
        "            \"üîç **Meta-Learning** - Final estimator learns from base model outputs\"\n",
        "        ]\n",
        "\n",
        "        print(\"Key Insights:\")\n",
        "        for insight in insights:\n",
        "            print(f\"  ‚Ä¢ {insight}\")\n",
        "\n",
        "# =============================================================================\n",
        "# READY-TO-RUN INTEGRATION\n",
        "# =============================================================================\n",
        "\n",
        "def run_stacking_xai():\n",
        "    \"\"\"\n",
        "    Ready-to-run function for Stacking Ensemble XAI\n",
        "    Uses exact same data pipeline as your ensemble code\n",
        "    \"\"\"\n",
        "    # Your Stacking Ensemble model (from ensemble code)\n",
        "    stacking_model = original_stacking  # Replace with your trained model\n",
        "\n",
        "    # Your data from ensemble code (already processed through RF selector)\n",
        "    # X_train, X_test, y_train, y_test from ensemble code\n",
        "\n",
        "    if stacking_model is None:\n",
        "        print(\"‚ùå Please train Stacking Ensemble model first\")\n",
        "        return\n",
        "\n",
        "    print(\"üîß Initializing Stacking Ensemble XAI...\")\n",
        "\n",
        "    # Initialize XAI\n",
        "    stacking_xai = StackingEnsembleXAI(\n",
        "        stacking_model=stacking_model,\n",
        "        X_train=X_train,  # From ensemble code\n",
        "        X_test=X_test,    # From ensemble code\n",
        "        y_train=y_train,  # From ensemble code\n",
        "        y_test=y_test     # From ensemble code\n",
        "    )\n",
        "\n",
        "    # Run complete analysis\n",
        "    results = stacking_xai.run_complete_xai()\n",
        "\n",
        "    return results\n",
        "\n",
        "# Uncomment to run:\n",
        "# stacking_results = run_stacking_xai()"
      ],
      "metadata": {
        "id": "VqWGZPlO7vqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üå≥ 2. Random Forest XAI (Traditional ML)"
      ],
      "metadata": {
        "id": "DtSxKFtr7wxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# RANDOM FOREST XAI - Uses Direct 12 Selected Features (No RF Selector)\n",
        "# =============================================================================\n",
        "\n",
        "import shap\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.inspection import partial_dependence\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class RandomForestXAI:\n",
        "    \"\"\"\n",
        "    XAI for Random Forest - Uses direct 12 selected features (no RF selector)\n",
        "    Maintains exact pipeline from traditional ML code\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, rf_model, X_train_scaled, X_test_scaled, y_train, y_test):\n",
        "        self.model = rf_model\n",
        "        self.X_train = X_train_scaled  # Already scaled from traditional ML code\n",
        "        self.X_test = X_test_scaled    # Already scaled from traditional ML code\n",
        "        self.y_train = y_train\n",
        "        self.y_test = y_test\n",
        "\n",
        "        # Direct 12 selected features from traditional ML code\n",
        "        self.feature_names = [\n",
        "            'blood pressure', 'specific gravity', 'albumin', 'sugar',\n",
        "            'blood glucose random', 'blood urea', 'sodium', 'potassium',\n",
        "            'hemoglobin', 'packed cell volume', 'white blood cell count',\n",
        "            'red blood cell count'\n",
        "        ]\n",
        "\n",
        "        self.results = {}\n",
        "\n",
        "        print(\"üå≥ Random Forest XAI Initialized\")\n",
        "        print(f\"üìä Using direct 12 selected features (no RF selector)\")\n",
        "        print(f\"üèóÔ∏è Model: {type(self.model).__name__}\")\n",
        "\n",
        "    def shap_analysis_rf(self):\n",
        "        \"\"\"SHAP analysis for Random Forest (TreeExplainer)\"\"\"\n",
        "        print(\"\\nüìä SHAP Analysis - Random Forest\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        try:\n",
        "            # TreeExplainer for Random Forest (optimal for tree models)\n",
        "            explainer = shap.TreeExplainer(self.model)\n",
        "            shap_values = explainer.shap_values(self.X_test[:100])\n",
        "\n",
        "            # Handle binary classification\n",
        "            if isinstance(shap_values, list):\n",
        "                shap_values = shap_values[1]  # Use class 1 (CKD)\n",
        "\n",
        "            # 1. Summary Plot\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            shap.summary_plot(shap_values, self.X_test[:100], feature_names=self.feature_names, show=False)\n",
        "            plt.title('SHAP Summary - Random Forest\\n(Impact on CKD Prediction)',\n",
        "                     fontsize=16, fontweight='bold', pad=20)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # 2. Bar Plot\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            shap.summary_plot(shap_values, self.X_test[:100], feature_names=self.feature_names,\n",
        "                             plot_type=\"bar\", show=False)\n",
        "            plt.title('SHAP Feature Importance - Random Forest',\n",
        "                     fontsize=16, fontweight='bold', pad=20)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # 3. Force Plot for first instance\n",
        "            plt.figure(figsize=(12, 4))\n",
        "            shap.force_plot(explainer.expected_value[1], shap_values[0],\n",
        "                          self.X_test[0], feature_names=self.feature_names, matplotlib=True, show=False)\n",
        "            plt.title('SHAP Force Plot - First Instance\\nRandom Forest',\n",
        "                     fontsize=14, fontweight='bold')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # 4. Dependence plots for top features\n",
        "            top_features_idx = np.argsort(np.mean(np.abs(shap_values), axis=0))[-3:]\n",
        "            for feature_idx in top_features_idx:\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                shap.dependence_plot(feature_idx, shap_values, self.X_test[:100],\n",
        "                                   feature_names=self.feature_names, show=False)\n",
        "                plt.title(f'SHAP Dependence Plot - {self.feature_names[feature_idx]}',\n",
        "                         fontsize=14, fontweight='bold')\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "            # Calculate feature importance\n",
        "            mean_abs_shap = np.mean(np.abs(shap_values), axis=0)\n",
        "            importance_df = pd.DataFrame({\n",
        "                'Feature': self.feature_names,\n",
        "                'SHAP_Importance': mean_abs_shap\n",
        "            }).sort_values('SHAP_Importance', ascending=False)\n",
        "\n",
        "            print(\"üîù Top Features (Random Forest):\")\n",
        "            print(importance_df.head(10).round(4))\n",
        "\n",
        "            # Compare with built-in feature importance\n",
        "            if hasattr(self.model, 'feature_importances_'):\n",
        "                builtin_importance = pd.DataFrame({\n",
        "                    'Feature': self.feature_names,\n",
        "                    'BuiltIn_Importance': self.model.feature_importances_\n",
        "                }).sort_values('BuiltIn_Importance', ascending=False)\n",
        "\n",
        "                print(\"\\nüîç Built-in Feature Importance:\")\n",
        "                print(builtin_importance.head(10).round(4))\n",
        "\n",
        "            self.results['shap'] = {\n",
        "                'explainer': explainer,\n",
        "                'shap_values': shap_values,\n",
        "                'importance_df': importance_df\n",
        "            }\n",
        "\n",
        "            return explainer, shap_values, importance_df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå SHAP failed for Random Forest: {str(e)}\")\n",
        "            return None, None, None\n",
        "\n",
        "    def lime_analysis_rf(self, instance_indices=[0, 1, 2]):\n",
        "        \"\"\"LIME analysis for Random Forest\"\"\"\n",
        "        print(f\"\\nüéØ LIME Local Explanations - Random Forest\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        try:\n",
        "            # Create LIME explainer\n",
        "            explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "                self.X_train,\n",
        "                feature_names=self.feature_names,\n",
        "                class_names=['No CKD', 'CKD'],\n",
        "                mode='classification',\n",
        "                random_state=42,\n",
        "                verbose=False\n",
        "            )\n",
        "\n",
        "            for instance_idx in instance_indices:\n",
        "                if instance_idx >= len(self.X_test):\n",
        "                    continue\n",
        "\n",
        "                # Explain instance\n",
        "                exp = explainer.explain_instance(\n",
        "                    self.X_test[instance_idx],\n",
        "                    self.model.predict_proba,\n",
        "                    num_features=len(self.feature_names),\n",
        "                    top_labels=1\n",
        "                )\n",
        "\n",
        "                # Get prediction details\n",
        "                actual_label = \"CKD\" if self.y_test[instance_idx] == 1 else \"No CKD\"\n",
        "                prediction_proba = self.model.predict_proba(self.X_test[instance_idx:instance_idx+1])[0]\n",
        "                predicted_label = \"CKD\" if np.argmax(prediction_proba) == 1 else \"No CKD\"\n",
        "\n",
        "                print(f\"\\nüìã Instance {instance_idx}:\")\n",
        "                print(f\"   Actual: {actual_label}, Predicted: {predicted_label}\")\n",
        "                print(f\"   Confidence: No CKD: {prediction_proba[0]:.3f}, CKD: {prediction_proba[1]:.3f}\")\n",
        "\n",
        "                # Plot explanation\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                exp.as_pyplot_figure()\n",
        "                plt.title(f'LIME - Random Forest (Instance {instance_idx})\\nActual: {actual_label}, Predicted: {predicted_label}',\n",
        "                         fontsize=12, fontweight='bold')\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "                # Print top features\n",
        "                print(\"   Top Feature Contributions:\")\n",
        "                for feature, weight in exp.local_exp[1][:5]:\n",
        "                    feature_name = self.feature_names[feature]\n",
        "                    direction = \"increases\" if weight > 0 else \"decreases\"\n",
        "                    print(f\"     {feature_name}: {weight:+.3f} ({direction} CKD risk)\")\n",
        "\n",
        "            self.results['lime'] = explainer\n",
        "            return explainer\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå LIME failed for Random Forest: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def traditional_feature_importance(self):\n",
        "        \"\"\"Traditional feature importance methods for Random Forest\"\"\"\n",
        "        print(f\"\\nüå≥ Traditional Feature Importance - Random Forest\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        try:\n",
        "            if hasattr(self.model, 'feature_importances_'):\n",
        "                # Create feature importance plot\n",
        "                importance_df = pd.DataFrame({\n",
        "                    'Feature': self.feature_names,\n",
        "                    'Importance': self.model.feature_importances_\n",
        "                }).sort_values('Importance', ascending=True)\n",
        "\n",
        "                plt.figure(figsize=(12, 8))\n",
        "                plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')\n",
        "                plt.xlabel('Feature Importance')\n",
        "                plt.title('Traditional Feature Importance - Random Forest', fontsize=16, fontweight='bold')\n",
        "                plt.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "                # Add value labels\n",
        "                for i, v in enumerate(importance_df['Importance']):\n",
        "                    plt.text(v + 0.01, i, f'{v:.3f}', va='center', fontsize=10)\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "                print(\"üìä Traditional Feature Importance:\")\n",
        "                print(importance_df.sort_values('Importance', ascending=False).round(4))\n",
        "\n",
        "                return importance_df\n",
        "            else:\n",
        "                print(\"‚ùå Model doesn't have feature_importances_ attribute\")\n",
        "                return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Traditional importance failed: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def run_complete_xai(self):\n",
        "        \"\"\"Run complete XAI pipeline for Random Forest\"\"\"\n",
        "        print(\"\\n\" + \"üå≥\" * 20)\n",
        "        print(\"RANDOM FOREST COMPLETE XAI ANALYSIS\")\n",
        "        print(\"üå≥\" * 20)\n",
        "\n",
        "        # 1. SHAP Analysis\n",
        "        self.shap_analysis_rf()\n",
        "\n",
        "        # 2. Traditional Feature Importance\n",
        "        self.traditional_feature_importance()\n",
        "\n",
        "        # 3. LIME Analysis\n",
        "        self.lime_analysis_rf()\n",
        "\n",
        "        # 4. Generate insights\n",
        "        self._generate_rf_insights()\n",
        "\n",
        "        print(f\"\\n‚úÖ RANDOM FOREST XAI COMPLETED!\")\n",
        "        return self.results\n",
        "\n",
        "    def _generate_rf_insights(self):\n",
        "        \"\"\"Generate specific insights for Random Forest\"\"\"\n",
        "        print(f\"\\nüè• RANDOM FOREST CLINICAL INSIGHTS\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        insights = [\n",
        "            \"üå≥ **Tree-based Interpretation** - Naturally interpretable feature splits\",\n",
        "            \"üìä **Stable Importance** - Robust feature importance rankings\",\n",
        "            \"üéØ **Direct Relationships** - Clear feature-decision relationships\",\n",
        "            \"üõ°Ô∏è **Overfitting Resistance** - Less prone to overfitting than deep models\"\n",
        "        ]\n",
        "\n",
        "        print(\"Key Insights:\")\n",
        "        for insight in insights:\n",
        "            print(f\"  ‚Ä¢ {insight}\")\n",
        "\n",
        "# =============================================================================\n",
        "# READY-TO-RUN INTEGRATION\n",
        "# =============================================================================\n",
        "\n",
        "def run_rf_xai():\n",
        "    \"\"\"\n",
        "    Ready-to-run function for Random Forest XAI\n",
        "    Uses exact same data pipeline as your traditional ML code\n",
        "    \"\"\"\n",
        "    # Your Random Forest model (from traditional ML code)\n",
        "    rf_model = models['Random Forest']  # Replace with your trained model\n",
        "\n",
        "    # Your scaled data from traditional ML code\n",
        "    # X_train_res_scaled, X_test_scaled, y_train_res, y_test from traditional ML code\n",
        "\n",
        "    if rf_model is None:\n",
        "        print(\"‚ùå Please train Random Forest model first\")\n",
        "        return\n",
        "\n",
        "    print(\"üîß Initializing Random Forest XAI...\")\n",
        "\n",
        "    # Initialize XAI\n",
        "    rf_xai = RandomForestXAI(\n",
        "        rf_model=rf_model,\n",
        "        X_train_scaled=X_train_res_scaled,  # From traditional ML code\n",
        "        X_test_scaled=X_test_scaled,        # From traditional ML code\n",
        "        y_train=y_train_res,                # From traditional ML code (after SMOTE)\n",
        "        y_test=y_test                       # From traditional ML code\n",
        "    )\n",
        "\n",
        "    # Run complete analysis\n",
        "    results = rf_xai.run_complete_xai()\n",
        "\n",
        "    return results\n",
        "\n",
        "# Uncomment to run:\n",
        "# rf_results = run_rf_xai()"
      ],
      "metadata": {
        "id": "UU2skezQ72A3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß† 3. Deep NN Bottleneck XAI"
      ],
      "metadata": {
        "id": "YBwgJ_Zj74rY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# DEEP NN BOTTLENECK XAI - Maintains Exact Architecture & RF Selector Pipeline\n",
        "# =============================================================================\n",
        "\n",
        "import shap\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class DNNBottleneckXAI:\n",
        "    \"\"\"\n",
        "    XAI for Deep NN Bottleneck - Maintains exact architecture and RF selector pipeline\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dnn_model, X_train, X_test, y_train, y_test):\n",
        "        self.model = dnn_model\n",
        "        self.X_train = X_train  # Already processed through RF selector\n",
        "        self.X_test = X_test    # Already processed through RF selector\n",
        "        self.y_train = y_train\n",
        "        self.y_test = y_test\n",
        "\n",
        "        # Feature names from RF selector (12 features)\n",
        "        self.feature_names = [\n",
        "            'blood pressure', 'specific gravity', 'albumin', 'sugar',\n",
        "            'blood glucose random', 'blood urea', 'sodium', 'potassium',\n",
        "            'hemoglobin', 'packed cell volume', 'white blood cell count',\n",
        "            'red blood cell count'\n",
        "        ]\n",
        "\n",
        "        self.results = {}\n",
        "\n",
        "        print(\"üß† Deep NN Bottleneck XAI Initialized\")\n",
        "        print(f\"üìä Using {len(self.feature_names)} RF-selected features\")\n",
        "        print(f\"üèóÔ∏è Model: {self.model.count_params():,} parameters\")\n",
        "        print(f\"üìê Architecture: {len(self.model.layers)} layers\")\n",
        "\n",
        "    def shap_analysis_dnn(self):\n",
        "        \"\"\"SHAP analysis for Deep Neural Network (DeepExplainer)\"\"\"\n",
        "        print(\"\\nüìä SHAP Analysis - Deep NN Bottleneck\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        try:\n",
        "            # Use DeepExplainer for neural networks\n",
        "            background = self.X_train[np.random.choice(len(self.X_train), 50, replace=False)]\n",
        "            explainer = shap.DeepExplainer(self.model, background)\n",
        "            shap_values = explainer.shap_values(self.X_test[:50])\n",
        "\n",
        "            # Handle output format (DNN with softmax output)\n",
        "            if isinstance(shap_values, list):\n",
        "                shap_values = shap_values[0]  # For binary classification\n",
        "\n",
        "            # 1. Summary Plot\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            shap.summary_plot(shap_values, self.X_test[:50], feature_names=self.feature_names, show=False)\n",
        "            plt.title('SHAP Summary - Deep NN Bottleneck\\n(Impact on CKD Prediction)',\n",
        "                     fontsize=16, fontweight='bold', pad=20)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # 2. Bar Plot\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            shap.summary_plot(shap_values, self.X_test[:50], feature_names=self.feature_names,\n",
        "                             plot_type=\"bar\", show=False)\n",
        "            plt.title('SHAP Feature Importance - Deep NN Bottleneck',\n",
        "                     fontsize=16, fontweight='bold', pad=20)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # Calculate feature importance\n",
        "            mean_abs_shap = np.mean(np.abs(shap_values), axis=0)\n",
        "            importance_df = pd.DataFrame({\n",
        "                'Feature': self.feature_names,\n",
        "                'SHAP_Importance': mean_abs_shap\n",
        "            }).sort_values('SHAP_Importance', ascending=False)\n",
        "\n",
        "            print(\"üîù Top Features (Deep NN Bottleneck):\")\n",
        "            print(importance_df.head(10).round(4))\n",
        "\n",
        "            self.results['shap'] = {\n",
        "                'explainer': explainer,\n",
        "                'shap_values': shap_values,\n",
        "                'importance_df': importance_df\n",
        "            }\n",
        "\n",
        "            return explainer, shap_values, importance_df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå SHAP DeepExplainer failed: {str(e)}\")\n",
        "            # Fallback to KernelSHAP\n",
        "            return self._shap_fallback_dnn()\n",
        "\n",
        "    def _shap_fallback_dnn(self):\n",
        "        \"\"\"Fallback SHAP method for DNN using KernelSHAP\"\"\"\n",
        "        try:\n",
        "            print(\"üîÑ Falling back to KernelSHAP for DNN...\")\n",
        "\n",
        "            def predict_wrapper(x):\n",
        "                return self.model.predict(x, verbose=0)\n",
        "\n",
        "            explainer = shap.KernelExplainer(predict_wrapper, self.X_train[:50])\n",
        "            shap_values = explainer.shap_values(self.X_test[:50])\n",
        "\n",
        "            if isinstance(shap_values, list):\n",
        "                shap_values = shap_values[0]\n",
        "\n",
        "            # Create plots\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            shap.summary_plot(shap_values, self.X_test[:50], feature_names=self.feature_names, show=False)\n",
        "            plt.title('SHAP Summary - Deep NN Bottleneck (KernelSHAP)',\n",
        "                     fontsize=16, fontweight='bold', pad=20)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            shap.summary_plot(shap_values, self.X_test[:50], feature_names=self.feature_names,\n",
        "                             plot_type=\"bar\", show=False)\n",
        "            plt.title('SHAP Feature Importance - Deep NN Bottleneck',\n",
        "                     fontsize=16, fontweight='bold', pad=20)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            mean_abs_shap = np.mean(np.abs(shap_values), axis=0)\n",
        "            importance_df = pd.DataFrame({\n",
        "                'Feature': self.feature_names,\n",
        "                'SHAP_Importance': mean_abs_shap\n",
        "            }).sort_values('SHAP_Importance', ascending=False)\n",
        "\n",
        "            print(\"üîù Top Features (Deep NN Bottleneck - KernelSHAP):\")\n",
        "            print(importance_df.head(10).round(4))\n",
        "\n",
        "            self.results['shap'] = {\n",
        "                'explainer': explainer,\n",
        "                'shap_values': shap_values,\n",
        "                'importance_df': importance_df\n",
        "            }\n",
        "\n",
        "            return explainer, shap_values, importance_df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå KernelSHAP also failed: {str(e)}\")\n",
        "            return None, None, None\n",
        "\n",
        "    def layer_activation_analysis(self, instance_idx=0):\n",
        "        \"\"\"Analyze layer activations to understand internal representations\"\"\"\n",
        "        print(f\"\\nüîç Layer Activation Analysis - Deep NN Bottleneck\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        try:\n",
        "            # Create intermediate models for each layer\n",
        "            layer_outputs = [layer.output for layer in self.model.layers[1:]]  # Skip input layer\n",
        "            activation_model = Model(inputs=self.model.input, outputs=layer_outputs)\n",
        "\n",
        "            # Get activations for a specific instance\n",
        "            instance = self.X_test[instance_idx:instance_idx+1]\n",
        "            activations = activation_model.predict(instance, verbose=0)\n",
        "\n",
        "            # Plot activations for each layer\n",
        "            fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
        "            axes = axes.ravel()\n",
        "\n",
        "            for i, (activation, layer) in enumerate(zip(activations, self.model.layers[1:])):\n",
        "                if i >= len(axes):\n",
        "                    break\n",
        "\n",
        "                # Flatten activation for hidden layers\n",
        "                activation_flat = activation.flatten()\n",
        "\n",
        "                # Plot activation distribution\n",
        "                axes[i].hist(activation_flat, bins=50, alpha=0.7, color=f'C{i}', edgecolor='black')\n",
        "                axes[i].set_title(f'Layer {i+1}: {layer.name}\\n{layer.units} units', fontweight='bold')\n",
        "                axes[i].set_xlabel('Activation Value')\n",
        "                axes[i].set_ylabel('Frequency')\n",
        "                axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "                # Add statistics\n",
        "                mean_val = activation_flat.mean()\n",
        "                std_val = activation_flat.std()\n",
        "                axes[i].axvline(mean_val, color='red', linestyle='--',\n",
        "                              label=f'Mean: {mean_val:.3f}')\n",
        "                axes[i].axvline(mean_val + std_val, color='orange', linestyle='--',\n",
        "                              label=f'¬±1 STD', alpha=0.7)\n",
        "                axes[i].axvline(mean_val - std_val, color='orange', linestyle='--', alpha=0.7)\n",
        "                axes[i].legend(fontsize=8)\n",
        "\n",
        "            # Remove empty subplots\n",
        "            for i in range(len(activations), len(axes)):\n",
        "                fig.delaxes(axes[i])\n",
        "\n",
        "            plt.suptitle('Layer Activation Analysis - Deep NN Bottleneck\\n(Internal Representation Patterns)',\n",
        "                        fontsize=16, fontweight='bold')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # Analyze bottleneck layer specifically\n",
        "            bottleneck_idx = 6  # Based on your architecture (64-unit layer)\n",
        "            if len(activations) > bottleneck_idx:\n",
        "                bottleneck_activations = activations[bottleneck_idx].flatten()\n",
        "\n",
        "                plt.figure(figsize=(12, 6))\n",
        "                plt.hist(bottleneck_activations, bins=50, alpha=0.7, color='purple', edgecolor='black')\n",
        "                plt.title('Bottleneck Layer Activations (64 units)\\nCompressed Feature Representation',\n",
        "                         fontsize=14, fontweight='bold')\n",
        "                plt.xlabel('Activation Value')\n",
        "                plt.ylabel('Frequency')\n",
        "                plt.grid(True, alpha=0.3)\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "                print(f\"üìä Bottleneck Layer Statistics:\")\n",
        "                print(f\"   ‚Ä¢ Mean activation: {bottleneck_activations.mean():.4f}\")\n",
        "                print(f\"   ‚Ä¢ Std activation: {bottleneck_activations.std():.4f}\")\n",
        "                print(f\"   ‚Ä¢ Activation range: [{bottleneck_activations.min():.4f}, {bottleneck_activations.max():.4f}]\")\n",
        "\n",
        "            self.results['activations'] = activations\n",
        "            return activations\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Activation analysis failed: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def lime_analysis_dnn(self, instance_indices=[0, 1, 2]):\n",
        "        \"\"\"LIME analysis for Deep Neural Network\"\"\"\n",
        "        print(f\"\\nüéØ LIME Local Explanations - Deep NN Bottleneck\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        try:\n",
        "            # Create LIME explainer\n",
        "            explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "                self.X_train,\n",
        "                feature_names=self.feature_names,\n",
        "                class_names=['No CKD', 'CKD'],\n",
        "                mode='classification',\n",
        "                random_state=42,\n",
        "                verbose=False\n",
        "            )\n",
        "\n",
        "            for instance_idx in instance_indices:\n",
        "                if instance_idx >= len(self.X_test):\n",
        "                    continue\n",
        "\n",
        "                # Explain instance\n",
        "                exp = explainer.explain_instance(\n",
        "                    self.X_test[instance_idx],\n",
        "                    lambda x: self.model.predict(x, verbose=0),  # DNN prediction\n",
        "                    num_features=len(self.feature_names),\n",
        "                    top_labels=1\n",
        "                )\n",
        "\n",
        "                # Get prediction details\n",
        "                actual_label = \"CKD\" if self.y_test[instance_idx] == 1 else \"No CKD\"\n",
        "                prediction_proba = self.model.predict(self.X_test[instance_idx:instance_idx+1], verbose=0)[0]\n",
        "                predicted_label = \"CKD\" if np.argmax(prediction_proba) == 1 else \"No CKD\"\n",
        "\n",
        "                print(f\"\\nüìã Instance {instance_idx}:\")\n",
        "                print(f\"   Actual: {actual_label}, Predicted: {predicted_label}\")\n",
        "                print(f\"   Confidence: No CKD: {prediction_proba[0]:.3f}, CKD: {prediction_proba[1]:.3f}\")\n",
        "\n",
        "                # Plot explanation\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                exp.as_pyplot_figure()\n",
        "                plt.title(f'LIME - Deep NN Bottleneck (Instance {instance_idx})\\nActual: {actual_label}, Predicted: {predicted_label}',\n",
        "                         fontsize=12, fontweight='bold')\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "                # Print top features\n",
        "                print(\"   Top Feature Contributions:\")\n",
        "                for feature, weight in exp.local_exp[1][:5]:\n",
        "                    feature_name = self.feature_names[feature]\n",
        "                    direction = \"increases\" if weight > 0 else \"decreases\"\n",
        "                    print(f\"     {feature_name}: {weight:+.3f} ({direction} CKD risk)\")\n",
        "\n",
        "            self.results['lime'] = explainer\n",
        "            return explainer\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå LIME failed for DNN: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def run_complete_xai(self):\n",
        "        \"\"\"Run complete XAI pipeline for Deep NN Bottleneck\"\"\"\n",
        "        print(\"\\n\" + \"üß†\" * 20)\n",
        "        print(\"DEEP NN BOTTLENECK COMPLETE XAI ANALYSIS\")\n",
        "        print(\"üß†\" * 20)\n",
        "\n",
        "        # 1. SHAP Analysis\n",
        "        self.shap_analysis_dnn()\n",
        "\n",
        "        # 2. Layer Activation Analysis\n",
        "        self.layer_activation_analysis()\n",
        "\n",
        "        # 3. LIME Analysis\n",
        "        self.lime_analysis_dnn()\n",
        "\n",
        "        # 4. Generate insights\n",
        "        self._generate_dnn_insights()\n",
        "\n",
        "        print(f\"\\n‚úÖ DEEP NN BOTTLENECK XAI COMPLETED!\")\n",
        "        return self.results\n",
        "\n",
        "    def _generate_dnn_insights(self):\n",
        "        \"\"\"Generate specific insights for Deep NN Bottleneck\"\"\"\n",
        "        print(f\"\\nüè• DEEP NN BOTTLENECK CLINICAL INSIGHTS\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        insights = [\n",
        "            \"üß† **Complex Pattern Detection** - Captures non-linear relationships between biomarkers\",\n",
        "            \"üîÑ **Bottleneck Architecture** - Compresses information then expands for robust features\",\n",
        "            \"üìä **Feature Learning** - Automatically learns relevant feature combinations\",\n",
        "            \"üéØ **High-Dimensional Reasoning** - Excels at complex medical pattern recognition\"\n",
        "        ]\n",
        "\n",
        "        print(\"Key Insights:\")\n",
        "        for insight in insights:\n",
        "            print(f\"  ‚Ä¢ {insight}\")\n",
        "\n",
        "# =============================================================================\n",
        "# READY-TO-RUN INTEGRATION\n",
        "# =============================================================================\n",
        "\n",
        "def run_dnn_bottleneck_xai():\n",
        "    \"\"\"\n",
        "    Ready-to-run function for Deep NN Bottleneck XAI\n",
        "    Uses exact same data pipeline as your DNN code\n",
        "    \"\"\"\n",
        "    # Your DNN Bottleneck model (from DNN code)\n",
        "    dnn_model = bottleneck_model  # Replace with your trained model\n",
        "\n",
        "    # Your data from DNN code (already processed through RF selector)\n",
        "    # X_train, X_test, y_train, y_test from DNN code\n",
        "\n",
        "    if dnn_model is None:\n",
        "        print(\"‚ùå Please train Deep NN Bottleneck model first\")\n",
        "        return\n",
        "\n",
        "    print(\"üîß Initializing Deep NN Bottleneck XAI...\")\n",
        "\n",
        "    # Initialize XAI\n",
        "    dnn_xai = DNNBottleneckXAI(\n",
        "        dnn_model=dnn_model,\n",
        "        X_train=X_train,  # From DNN code\n",
        "        X_test=X_test,    # From DNN code\n",
        "        y_train=y_train,  # From DNN code\n",
        "        y_test=y_test     # From DNN code\n",
        "    )\n",
        "\n",
        "    # Run complete analysis\n",
        "    results = dnn_xai.run_complete_xai()\n",
        "\n",
        "    return results\n",
        "\n",
        "# Uncomment to run:\n",
        "# dnn_results = run_dnn_bottleneck_xai()"
      ],
      "metadata": {
        "id": "ksxzabGr77Lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üöÄ 4. Enhanced Stacking XAI"
      ],
      "metadata": {
        "id": "_KKBUcl_776B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ENHANCED STACKING XAI - Maintains Exact RF Selector Pipeline\n",
        "# =============================================================================\n",
        "\n",
        "import shap\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.inspection import partial_dependence\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class EnhancedStackingXAI:\n",
        "    \"\"\"\n",
        "    XAI for Enhanced Stacking - Maintains exact RF selector pipeline from ensemble code\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, enhanced_stacking_model, X_train, X_test, y_train, y_test):\n",
        "        self.model = enhanced_stacking_model\n",
        "        self.X_train = X_train  # Already processed through RF selector\n",
        "        self.X_test = X_test    # Already processed through RF selector\n",
        "        self.y_train = y_train\n",
        "        self.y_test = y_test\n",
        "\n",
        "        # Feature names from RF selector (12 features)\n",
        "        self.feature_names = [\n",
        "            'blood pressure', 'specific gravity', 'albumin', 'sugar',\n",
        "            'blood glucose random', 'blood urea', 'sodium', 'potassium',\n",
        "            'hemoglobin', 'packed cell volume', 'white blood cell count',\n",
        "            'red blood cell count'\n",
        "        ]\n",
        "\n",
        "        self.results = {}\n",
        "\n",
        "        print(\"üöÄ Enhanced Stacking XAI Initialized\")\n",
        "        print(f\"üìä Using {len(self.feature_names)} RF-selected features\")\n",
        "        print(f\"üèóÔ∏è Model: {type(self.model).__name__}\")\n",
        "        print(f\"‚≠ê Highest Accuracy Model (0.877)\")\n",
        "\n",
        "    def shap_analysis_enhanced_stacking(self):\n",
        "        \"\"\"SHAP analysis for Enhanced Stacking Ensemble\"\"\"\n",
        "        print(\"\\nüìä SHAP Analysis - Enhanced Stacking\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        try:\n",
        "            # Sample for efficiency\n",
        "            X_sample = self.X_test[:100]\n",
        "\n",
        "            # KernelSHAP for complex ensembles\n",
        "            explainer = shap.KernelExplainer(self.model.predict_proba, self.X_train[:50])\n",
        "            shap_values = explainer.shap_values(X_sample)\n",
        "\n",
        "            # Handle binary classification output\n",
        "            if isinstance(shap_values, list):\n",
        "                shap_values = shap_values[1]  # Use class 1 (CKD)\n",
        "\n",
        "            # 1. Summary Plot\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            shap.summary_plot(shap_values, X_sample, feature_names=self.feature_names, show=False)\n",
        "            plt.title('SHAP Summary - Enhanced Stacking\\n(Highest Accuracy Model)',\n",
        "                     fontsize=16, fontweight='bold', pad=20)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # 2. Bar Plot\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            shap.summary_plot(shap_values, X_sample, feature_names=self.feature_names,\n",
        "                             plot_type=\"bar\", show=False)\n",
        "            plt.title('SHAP Feature Importance - Enhanced Stacking',\n",
        "                     fontsize=16, fontweight='bold', pad=20)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # 3. Multiple Force Plots for different prediction types\n",
        "            predictions = self.model.predict_proba(X_sample[:3])[:, 1]\n",
        "\n",
        "            for i in range(min(3, len(X_sample))):\n",
        "                plt.figure(figsize=(12, 4))\n",
        "                shap.force_plot(explainer.expected_value[1], shap_values[i],\n",
        "                              X_sample[i], feature_names=self.feature_names, matplotlib=True, show=False)\n",
        "                pred_type = \"High Confidence\" if predictions[i] > 0.7 else \"Low Confidence\" if predictions[i] < 0.3 else \"Medium Confidence\"\n",
        "                plt.title(f'SHAP Force Plot - Instance {i} ({pred_type})',\n",
        "                         fontsize=14, fontweight='bold')\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "            # Calculate feature importance\n",
        "            mean_abs_shap = np.mean(np.abs(shap_values), axis=0)\n",
        "            importance_df = pd.DataFrame({\n",
        "                'Feature': self.feature_names,\n",
        "                'SHAP_Importance': mean_abs_shap\n",
        "            }).sort_values('SHAP_Importance', ascending=False)\n",
        "\n",
        "            print(\"üîù Top Features (Enhanced Stacking - Highest Accuracy):\")\n",
        "            print(importance_df.head(10).round(4))\n",
        "\n",
        "            self.results['shap'] = {\n",
        "                'explainer': explainer,\n",
        "                'shap_values': shap_values,\n",
        "                'importance_df': importance_df\n",
        "            }\n",
        "\n",
        "            return explainer, shap_values, importance_df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå SHAP failed for Enhanced Stacking: {str(e)}\")\n",
        "            return None, None, None\n",
        "\n",
        "    def lime_analysis_enhanced_stacking(self, instance_indices=[0, 1, 2]):\n",
        "        \"\"\"LIME analysis for Enhanced Stacking\"\"\"\n",
        "        print(f\"\\nüéØ LIME Local Explanations - Enhanced Stacking\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        try:\n",
        "            # Create LIME explainer\n",
        "            explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "                self.X_train,\n",
        "                feature_names=self.feature_names,\n",
        "                class_names=['No CKD', 'CKD'],\n",
        "                mode='classification',\n",
        "                random_state=42,\n",
        "                verbose=False\n",
        "            )\n",
        "\n",
        "            for instance_idx in instance_indices:\n",
        "                if instance_idx >= len(self.X_test):\n",
        "                    continue\n",
        "\n",
        "                # Explain instance\n",
        "                exp = explainer.explain_instance(\n",
        "                    self.X_test[instance_idx],\n",
        "                    self.model.predict_proba,\n",
        "                    num_features=len(self.feature_names),\n",
        "                    top_labels=1\n",
        "                )\n",
        "\n",
        "                # Get prediction details\n",
        "                actual_label = \"CKD\" if self.y_test[instance_idx] == 1 else \"No CKD\"\n",
        "                prediction_proba = self.model.predict_proba(self.X_test[instance_idx:instance_idx+1])[0]\n",
        "                predicted_label = \"CKD\" if np.argmax(prediction_proba) == 1 else \"No CKD\"\n",
        "                confidence_level = \"High\" if max(prediction_proba) > 0.8 else \"Medium\" if max(prediction_proba) > 0.6 else \"Low\"\n",
        "\n",
        "                print(f\"\\nüìã Instance {instance_idx} ({confidence_level} Confidence):\")\n",
        "                print(f\"   Actual: {actual_label}, Predicted: {predicted_label}\")\n",
        "                print(f\"   Confidence: No CKD: {prediction_proba[0]:.3f}, CKD: {prediction_proba[1]:.3f}\")\n",
        "\n",
        "                # Plot explanation\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                exp.as_pyplot_figure()\n",
        "                plt.title(f'LIME - Enhanced Stacking (Instance {instance_idx})\\nActual: {actual_label}, Predicted: {predicted_label}',\n",
        "                         fontsize=12, fontweight='bold')\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "                # Print top features with clinical context\n",
        "                print(\"   Top Feature Contributions:\")\n",
        "                for feature, weight in exp.local_exp[1][:5]:\n",
        "                    feature_name = self.feature_names[feature]\n",
        "                    direction = \"increases\" if weight > 0 else \"decreases\"\n",
        "                    magnitude = \"strongly\" if abs(weight) > 0.1 else \"moderately\" if abs(weight) > 0.05 else \"slightly\"\n",
        "                    print(f\"     {feature_name}: {weight:+.3f} ({magnitude} {direction} CKD risk)\")\n",
        "\n",
        "            self.results['lime'] = explainer\n",
        "            return explainer\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå LIME failed for Enhanced Stacking: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def confidence_analysis(self):\n",
        "        \"\"\"Analyze prediction confidence for Enhanced Stacking\"\"\"\n",
        "        print(f\"\\nüéöÔ∏è Confidence Analysis - Enhanced Stacking\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        try:\n",
        "            # Get predictions\n",
        "            y_proba = self.model.predict_proba(self.X_test)[:, 1]\n",
        "            y_pred = (y_proba > 0.5).astype(int)\n",
        "\n",
        "            # Confidence distribution\n",
        "            plt.figure(figsize=(15, 5))\n",
        "\n",
        "            # 1. Overall confidence distribution\n",
        "            plt.subplot(1, 3, 1)\n",
        "            plt.hist(y_proba, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "            plt.axvline(x=0.5, color='red', linestyle='--', alpha=0.7, label='Decision Boundary')\n",
        "            plt.xlabel('Predicted Probability of CKD')\n",
        "            plt.ylabel('Frequency')\n",
        "            plt.title('Confidence Distribution\\nEnhanced Stacking', fontweight='bold')\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "            # 2. Confidence by actual class\n",
        "            plt.subplot(1, 3, 2)\n",
        "            ckd_confidences = y_proba[self.y_test == 1]\n",
        "            no_ckd_confidences = y_proba[self.y_test == 0]\n",
        "\n",
        "            plt.hist(no_ckd_confidences, bins=15, alpha=0.7, label='Actual No CKD', color='blue')\n",
        "            plt.hist(ckd_confidences, bins=15, alpha=0.7, label='Actual CKD', color='red')\n",
        "            plt.axvline(x=0.5, color='black', linestyle='--', alpha=0.7)\n",
        "            plt.xlabel('Predicted Probability of CKD')\n",
        "            plt.ylabel('Frequency')\n",
        "            plt.title('Confidence by Actual Class', fontweight='bold')\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "            # 3. Accuracy by confidence bins\n",
        "            plt.subplot(1, 3, 3)\n",
        "            confidence_bins = np.linspace(0, 1, 11)\n",
        "            accuracy_per_bin = []\n",
        "\n",
        "            for i in range(len(confidence_bins)-1):\n",
        "                mask = (y_proba >= confidence_bins[i]) & (y_proba < confidence_bins[i+1])\n",
        "                if np.sum(mask) > 0:\n",
        "                    accuracy = np.mean(y_pred[mask] == self.y_test[mask])\n",
        "                    accuracy_per_bin.append(accuracy)\n",
        "                else:\n",
        "                    accuracy_per_bin.append(0)\n",
        "\n",
        "            bin_centers = (confidence_bins[:-1] + confidence_bins[1:]) / 2\n",
        "            plt.plot(bin_centers, accuracy_per_bin, 'o-', linewidth=2, markersize=8, color='green')\n",
        "            plt.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Perfect Calibration')\n",
        "            plt.xlabel('Mean Confidence in Bin')\n",
        "            plt.ylabel('Accuracy in Bin')\n",
        "            plt.title('Confidence vs Accuracy', fontweight='bold')\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # Print confidence statistics\n",
        "            print(\"üìä Confidence Statistics:\")\n",
        "            print(f\"   ‚Ä¢ Average confidence: {np.mean(y_proba):.3f}\")\n",
        "            print(f\"   ‚Ä¢ Confidence std: {np.std(y_proba):.3f}\")\n",
        "            print(f\"   ‚Ä¢ High confidence predictions (>0.8): {np.mean(y_proba > 0.8):.1%}\")\n",
        "            print(f\"   ‚Ä¢ Low confidence predictions (<0.2): {np.mean(y_proba < 0.2):.1%}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Confidence analysis failed: {str(e)}\")\n",
        "\n",
        "    def run_complete_xai(self):\n",
        "        \"\"\"Run complete XAI pipeline for Enhanced Stacking\"\"\"\n",
        "        print(\"\\n\" + \"üöÄ\" * 20)\n",
        "        print(\"ENHANCED STACKING COMPLETE XAI ANALYSIS\")\n",
        "        print(\"üöÄ\" * 20)\n",
        "\n",
        "        # 1. SHAP Analysis\n",
        "        self.shap_analysis_enhanced_stacking()\n",
        "\n",
        "        # 2. LIME Analysis\n",
        "        self.lime_analysis_enhanced_stacking()\n",
        "\n",
        "        # 3. Confidence Analysis\n",
        "        self.confidence_analysis()\n",
        "\n",
        "        # 4. Generate insights\n",
        "        self._generate_enhanced_stacking_insights()\n",
        "\n",
        "        print(f\"\\n‚úÖ ENHANCED STACKING XAI COMPLETED!\")\n",
        "        return self.results\n",
        "\n",
        "    def _generate_enhanced_stacking_insights(self):\n",
        "        \"\"\"Generate specific insights for Enhanced Stacking\"\"\"\n",
        "        print(f\"\\nüè• ENHANCED STACKING CLINICAL INSIGHTS\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        insights = [\n",
        "            \"üöÄ **Highest Accuracy** - 0.877 accuracy with robust feature understanding\",\n",
        "            \"üîÑ **Enhanced Base Models** - Uses 5 diverse models for better coverage\",\n",
        "            \"üéØ **Meta-Learning Optimized** - Final estimator effectively combines predictions\",\n",
        "            \"üõ°Ô∏è **Clinical Reliability** - High confidence scores match actual accuracy\"\n",
        "        ]\n",
        "\n",
        "        print(\"Key Insights (Highest Performing Model):\")\n",
        "        for insight in insights:\n",
        "            print(f\"  ‚Ä¢ {insight}\")\n",
        "\n",
        "# =============================================================================\n",
        "# READY-TO-RUN INTEGRATION\n",
        "# =============================================================================\n",
        "\n",
        "def run_enhanced_stacking_xai():\n",
        "    \"\"\"\n",
        "    Ready-to-run function for Enhanced Stacking XAI\n",
        "    Uses exact same data pipeline as your ensemble code\n",
        "    \"\"\"\n",
        "    # Your Enhanced Stacking model (from ensemble code)\n",
        "    enhanced_model = enhanced_stacking  # Replace with your trained model\n",
        "\n",
        "    # Your data from ensemble code (already processed through RF selector)\n",
        "    # X_train, X_test, y_train, y_test from ensemble code\n",
        "\n",
        "    if enhanced_model is None:\n",
        "        print(\"‚ùå Please train Enhanced Stacking model first\")\n",
        "        return\n",
        "\n",
        "    print(\"üîß Initializing Enhanced Stacking XAI...\")\n",
        "\n",
        "    # Initialize XAI\n",
        "    enhanced_xai = EnhancedStackingXAI(\n",
        "        enhanced_stacking_model=enhanced_model,\n",
        "        X_train=X_train,  # From ensemble code\n",
        "        X_test=X_test,    # From ensemble code\n",
        "        y_train=y_train,  # From ensemble code\n",
        "        y_test=y_test     # From ensemble code\n",
        "    )\n",
        "\n",
        "    # Run complete analysis\n",
        "    results = enhanced_xai.run_complete_xai()\n",
        "\n",
        "    return results\n",
        "\n",
        "# Uncomment to run:\n",
        "# enhanced_results = run_enhanced_stacking_xai()"
      ],
      "metadata": {
        "id": "zmSqj7t97-sV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}